\documentclass[10pt,xcolor={table,dvipsnames}]{beamer} 		% carica automaticamente amsthm, amssymb, amsmath, graphicx

\usepackage{appendixnumberbeamer}		% Per numerare le slides in Beamer sì da non contare quelle di appendice
\usepackage[T1]{fontenc}				% codifica dei font
\usepackage[utf8]{inputenc}				% lettere accentate da tastiera
\usepackage[italian]{babel}				% lingua del documento
\usepackage[italian]{varioref}			% Per usare il comando \vref{label}, che dà dei collegamenti più dettagliati

% Load the custom style file
\usepackage{AndreaStyle}
% The file `AndreaStyle.sty` is stored in: `D:\Programmi e Applicazioni\texlive\texmf-local\tex\latex\local` for Windows.
% The file `AndreaStyle.sty` is stored in: `/usr/local/texlive/texmf-local/tex/latex/local` for Ubuntu (desktop).
% This won't work in Overleaf, until the AndreaStyle.sty file is added to the project

%\usepackage{braket} 					% Per il comando \Set, e altre (poche) cose
%\usepackage{textcomp}					% Dovrebbe aggiungere più simboli

\usepackage{algorithm}                  % Definisce l'ambiente algorithm (ha numero e didascalia)
\usepackage[beginLComment=//~,endLComment=~]{algpseudocodex} % Per scrivere pseudocodice

\usepackage{relsize}					% Per usare \mathbigger{} ecc

\usepackage{multirow}					% per usare il comando multirow
\usepackage{tabularx}					% per fare tabelle. Carica il pacchetto array, per gli array.
\usepackage{arydshln}					% per le linee tratteggiate nelle tabelle

\usepackage{nicematrix}

%\usepackage[rightcaption]{sidecap}		% Per mettere le didascalie di lato

%\usepackage{tikz}
\usepackage{fontawesome5}				% Aggiunge simboli da FontAwesome

\usepackage[many]{tcolorbox}
\usepackage{hyperref}					% Importante: hyperref va caricato nel documento.



%\setcounter{tocdepth}{1}	% profondità dell'indice

	% ------ TEOREMI CUSTOM ------
\theoremstyle{plain}					% Definisce ambienti per Teoremi, esercizi, corollari... Con lo stile adeguato
	\newtheorem{proposizione}{Proposizione}[section]
	\newtheorem*{proposizione*}{Proposizione}
	
	\newtheorem{teorema}{Teorema}[section]
	\newtheorem*{teorema*}{Teorema}
		
	%\newtheorem{lemma_es}{Lemma}[esercizio]
	%\newtheorem{lemma}{Lemma}[section]
	\newtheorem*{lemma*}{Lemma}
	\newtheorem{corollario}{Corollario}[section]


\theoremstyle{definition}				
	\newtheorem{definizione}{Definizione}[section]%[chapter]
	\newtheorem*{definizione*}{Definizione}	%definizione non numerata
	\newtheorem*{notazione}{Notazione}

\theoremstyle{remark}
	\newtheorem{oss}{Osservazione}[section]
	\newtheorem*{oss*}{Osservazione}
	
	% ------ COMANDI CUSTOM ------

% Define a command to create unnumbered footnotes
\let\svthefootnote\thefootnote
\textheight 1in
\newcommand\blankfootnote[1]{%
	\let\thefootnote\relax\footnotetext{#1}%
	\let\thefootnote\svthefootnote%
}

% Rinomina i "Require" ed "Ensure" dei listings creati con algorithmicx
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}


	% ------ COLORI CUSTOM ------

	
	
% TCOLORBOX 
\newtcolorbox{notitleblock}{
	enhanced,
	size = fbox,
	outer arc = 0mm,
	arc = 0mm,
	oversize, 
	colback = UniWhite, 
	frame hidden, 
	coltext = DMDarkBlue
}

	% ------- ALTRA BUROCRAZIA -------
\setbeamertemplate{page number in head/foot}[appendixframenumber] % Per non numerare le pagine in appendice
%\setbeamertemplate{theorems}[numbered]		% Per numerare i teoremi etc nella presentazione (una cosa che di solito non si fa)

% ========================================= INIZIO CODICE =========================================
	% Tema alternativo (Richiede XeLaTeX)
%\usetheme{SNSPisa}
%\setbeamercolor{blockcolor}{bg=UniWhite, fg=DMDarkBlue}
    % Tema solito 
\usetheme{Madrid}

\title[Seminario MA]{Backpropagation through Back substitution with a Backslash}
%\subtitle{Presentazione e dimostrazione della convergenza} 
\author{Andrea Marino}
\institute[DI UniPi]{Università di Pisa}
%\titlegraphic{\includegraphics[width=2cm]{Immagini/cherubino_black.eps}}
\date[\today]{Metodi di Approssimazione\newline Seminario di fine corso}

% --------------------- COME MOSTRARE APPENDICE --------------------
% ----------------------------- start ------------------------------
	% The sections begin with a table of contents slide in which all sections are shaded, but the 
	%	current section is visible
	% Each subsection begins with a table of contents slides in which the current section is shown, the 
	% 	current subsection is visible and the other subsections within the section are shaded
\AtBeginSection[] 					
{
	\begin{frame}
		\frametitle{Sommario}
		\tableofcontents[
		currentsection, % Only show the current section
		subsectionstyle=show/hide/hide % Show subsections of the current section, hide everything else
		] 
	\end{frame}
}

\AtBeginSubsection[]
{
	\begin{frame}
		\frametitle{Sommario}
		\tableofcontents[
		currentsection, % Only show the current section
		subsectionstyle=show/shaded/hide % Show the current subsection, shade others in the same section, hide everything else
		]
	\end{frame}
}
% ----------------------------- end ------------------------------



\begin{document}
	\begin{frame}[plain]
		\titlepage
	\end{frame}
	
	% Slide di sommario iniziale (La prima sezione inizia con un nuovo sommario, quindi decommentando il frame
	%	qui sotto ci saranno due slides di sommario, ma questa mostrerà tutte le sezioni (non le sottosezioni))
%\section*{Sommario}
%	\setcounter{tocdepth}{1}
%	\begin{frame}
%		\frametitle{Sommario}
%		\tableofcontents
%	\end{frame}	
%	\setcounter{tocdepth}{2}

\section{Introduzione}
	\begin{frame}{Introduzione}
		\textcolor{red}{\textbf{Da aggiustare}}
		
		\textbf{Obiettivo}: implementare la backpropagation, in Julia, usando il backslash.
		\medskip 
		
		\onslide<2->{Introducendo un tipo di dato per rappresentare funzioni lineari, dopo un minimo di setup per definire delle elementari operazioni su questo tipo di dato {\smaller (ma senza overloading del backslash!)}, il calcolo di $\grad{J}$ sarà dato da 
		\[
			\grad{J} = M^{\top}\qty((I - \tilde{L}) \,\backslash\, g)
		\]
		dove $M$ e $\tilde{L}$ sono matrici che hanno operatori come elementi.}
		%\smallskip 
		
		\onslide<3->{In particolare:}
		\begin{enumerate}
			\item<3-> Introdurremo i \emph{path weights} su un DAG, descrivendo il loro calcolo con il linguaggio dell'algebra lineare
			\item<4-> Tracceremo un collegamento tra entità e operazioni su \emph{computational graph} e concetti di automatic differentiation {\smaller (in particolare, \emph{path weights} $\equiv$ derivate totali)}
			\item<5-> Sarà possibile definire matrici di operatori {\smaller (aka il tipo di dato)}
			\item<6-> Risoluzione di un sistema lineare $\equiv$ calcolo di tutte le derivate totali
		\end{enumerate}
		
	\end{frame}

%    \begin{frame}
%    	{\hypertarget{frame:prova_0}{Slide 0}}
%    	Slide di test 0 (cfr. Appendice~\hyperlink{frame:prova_0:appendice}{\faHandPointRight})
%    \end{frame}
    
    
\section{Cammini, \emph{path weights} su DAG}
	\subsection{Definizioni e prime proprietà}
	\begin{frame}{Definizioni $\qquad 1/2$}{\emph{Path products} e \emph{path weights}}
		Consideriamo un DAG {\smaller (connesso!)} $\mathcal{G}$ e un cammino {\smaller (orientato!)} su $\mathcal{G}$.
		
		\begin{definizione}<2->
			Il \alert{path product} di un cammino su $\mathcal{G}$ è dato dal prodotto dei pesi degli archi che lo costituiscono
		\end{definizione}
		
		\onslide<3->{Assumiamo che in $\mathcal{G}$ non vi siano self loops. D'ufficio, il path product da un qualsiasi nodo a sé stesso è $1$.}
		
		\begin{definizione}<4->
			Un \alert{path weight} su $\mathcal{G}$ è il path product di un cammino da un nodo sorgente a un nodo pozzo
		\end{definizione}
		\medskip	
	
		\begin{oss}<5->
			Il ``prodotto'' è da intendersi nel senso di teoria dei gruppi. 
		\end{oss}

	\end{frame}
	
	\begin{frame}{Definizioni $\qquad 2/2$}{Matrice di adiacenza}
		Abuso di notazione: un nodo viene identificato con la sua etichetta $i\in[n]$%\coloneqq\qty{1,\dots,n}$.
		
		\begin{definizione}
			La \alert{matrice di adiacenza} $L$ di $\mathcal{G}$ è data da:
			\[
				\R{n}{n}\ni (L^{\top})_{i,j}= 
				\begin{dcases*}
					0 & {\smaller Non esiste un arco $i\to j$}\\
					w(i,j) & {\smaller Esiste un arco $i\to j$, con peso $w(i,j)$}
				\end{dcases*}
			\]
		\end{definizione}
		\medskip 
		
		\onslide<2->{Ricordiamo che $(L^{\top})^k$ accumula {\smaller ($\equiv$ somma)} i path products di lunghezza $k$:}
		\onslide<3->{$(L^{\top})^k_{i,j}$ contiene la somma dei pesi di tutti i cammini da $i$ a $j$ di lunghezza $k$.}
		

		
		\blankfootnote{$[n]\coloneqq\qty{1,\dots, n}$}
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:path_product_no_top_sort}{Matrice di adiacenza e \emph{path products}}}
		Se i nodi di $\mathcal{G}$ sono enumerati seguendo un ordinamento topologico, $L^{\top}$ è strettamente triangolare superiore, dunque nilpotente. \onslide<2->{Di conseguenza 
		\[
			(I-L^{\top})^{-1}=I + L^{\top} + \dots + (L^{\top})^{n-1}.
		\]}
		%\vspace*{-1em}
		\onslide<3->{\noindent Dunque $(I-L)^{-\!\top}$ contiene tutti i path products massimali, compresi i path weights.}
		\medskip
		
		\onslide<4->{Se i nodi sorgente sono $1,\dots,s$ e i nodi pozzo $n-p+1,\dots,n$, si ha
		\begin{equation*}\label{eqn:path_weights}
			\texttt{path\_weights}=
			\underbrace{
				\left\lgroup\begin{array}{cc}
					\multicolumn{2}{c}{\multirow{2}*{$I_s$}}\\
					& \\
					\hline
					\multicolumn{2}{c}{\multirow{2}*{$\mathlarger{0}$}} \\
					& \\
				\end{array}\right\rgroup^{\mathllap{\top}}
			}_{\text{sorgenti}}
			\!\qty(I - L)^{-\!\top}\!\!
			\underbrace{
				\left\lgroup\begin{array}{cc}
					\multicolumn{2}{c}{\multirow{3}*{$\mathlarger{0}$}} \\
					& \\
					& \\
					\hline
					\multicolumn{2}{c}{I_p}\\
				\end{array}\right\rgroup
			}_{\text{pozzi}}
		\end{equation*}}
		\vspace*{-1em}
		\begin{oss}<5->
			Il metodo è valido anche senza ordinamento topologico, basta selezionare le righe/colonne corrispondenti alle sorgenti/pozzi (cfr. Appendice~\hyperlink{frame:path_product_no_top_sort:appendice}{\faHandPointRight})
		\end{oss}
	\end{frame}
	
	% Versione della formula con NiceMatrix (più lenta)
%	\begin{frame}{title}
%		\[
%		\text{path weights}=
%		\begin{pNiceMatrix}%{cc}
%			\Block{2-3}<\large>{I_s} & & \\
%			& & \\
%			\Hline
%			\Block{2-3}<\large>{0} & & \\
%			& & \\
%			\CodeAfter
%			\UnderBrace[yshift=2pt]{4-1}{4-3}{\text{\smaller sorgenti}}
%		\end{pNiceMatrix}^{\mathllap{\top}}
%		\qty(I - L)^{-\!\top}
%		\begin{pNiceMatrix}%{cc}
%			\Block{3-2}<\large>{0} & \\
%			& \\
%			& \\
%			\Hline
%			\Block{1-2}{I_p} 	   & \\
%			\CodeAfter
%			\UnderBrace[yshift=2pt]{5-1}{5-2}{\text{\smaller pozzi}}
%		\end{pNiceMatrix}
%		\]
%	\end{frame}


	\begin{frame}{Esempio: DAG completo con quattro nodi}
		\begin{columns}
			\begin{column}{0.7\textwidth}
				\begin{figure}[ht]
					\centering
					\includegraphics[width=\textwidth]{Immagini/DAG_completo}
					\caption{\emph{Grafo, \emph{path weights}, matrice di incidenza e matrice dei \emph{path products}.} {\smaller (Immagine tratta dall'articolo originale)}}
				\end{figure}
			\end{column}
			\begin{column}{0.3\textwidth}
				\begin{align*}
					& \begin{multlined}
						(L^{\top})^2 = \\
						= \!\left(\begin{smallmatrix}
							\cdot & \cdot & a_1b_2 & a_1c_2+b_1c_3\\
							& \cdot & \cdot  & b_2c_3 \\
							& 	   & \cdot  & \cdot \\
							& 	   & 	    & \cdot 
						\end{smallmatrix}\right)
					\end{multlined} \\
					& (L^{\top})^3 = 
						\left(\begin{smallmatrix}
							\cdot & \cdot & \cdot & a_1b_2c_3\\
								  & \cdot & \cdot & \cdot \\
								  & 	  & \cdot & \cdot \\
								  & 	  & 	  & \cdot 
						\end{smallmatrix}\right)\\
				\end{align*}
			\end{column}
		\end{columns}
		
	\end{frame}

	\subsection{Calcolo dei \emph{path weights}}
	\begin{frame}
		{\hypertarget{frame:backsub_is_backprop}{Backpropagation come sostituzione all'indietro}}
		
		Quanto appena visto mostra che si può formulare il problema del calcolo dei \emph{path weights} in termini di algebra lineare.
		\smallskip
		
		\onslide<2->{Per l'associatività del prodotto, esistono diversi modi di calcolare i \emph{path weights}: dalle sorgenti ai pozzi {\smaller (\emph{forward})}, dai pozzi alle sorgenti {\smaller (\emph{backward})}.}
		
		\begin{block}<3->{}
			In generale, diversi modi di valutare l'espressione $\texttt{path\_weights}=\left(\begin{smallmatrix}I_s \\ 0\end{smallmatrix}\right)^{\!\!\top}(I-L)^{-\!\top}\left(\begin{smallmatrix}0 \\ I_p\end{smallmatrix}\right)$ corrispondono a diverse ``strategie'' per calcolare i \emph{path products}. (cfr. Appendice~\hyperlink{frame:different_orders:appendice}{\faHandPointRight}) 			
			\smallskip 
			
			\textbf{Idea}: a ciascun modo di ``inserire parentesi'' corrisponde un ordine di visita di $\mathcal{G}$. 
		\end{block}
		
		\onslide<4->{In particolare, calcolare \texttt{path\_weights} risolvendo $(I-L^{\top})x=\left(\begin{smallmatrix}0 \\ I_p\end{smallmatrix}\right)$ tramite sostituzione all'indietro corrisponde a un calcolo che procede dai pozzi alle sorgenti (cfr. Appendice~\hyperlink{frame:backsub_is_backprop:appendice}{\faHandPointRight}).}
		\smallskip 
		
		\onslide<5->{Questa strategia corrisponde alla backpropagation per calcolare path products su grafi computazionali.}
	\end{frame}
	

\section{Grafi computazionali}
	\subsection{Definizione, relazioni tra DAG e AD}
	\begin{frame}{Grafi computazionali}
		Un \emph{grafo computazionale} rappresenta la ``struttura'' di una funzione. Mostrando quali \emph{step} sono richiesti per la valutazione, un grafo computazionale può descrivere un algoritmo di calcolo.
		
		\begin{definizione}
			Un \alert{grafo computazionale} è un DAG etichettato, in cui:
			\begin{itemize}
				\item i \textbf{nodi sorgente} sono etichettati con nomi di variabili
				\item i \textbf{nodi interni}, oltre a nomi di variabili, contengono formule che dipendono dai nodi entranti
				\item ciascun \textbf{arco}  $i\to j$ è etichettato con $\pdv*{j}{i}$ {\smaller (ovvero la derivata della formula del nodo $j$ rispetto alla variabile $i$)}
			\end{itemize}
		\end{definizione}
		
		\textcolor{red}{Consideriamo grafi computazionali che derivano da algoritmi ``semplici''.}
	\end{frame}
	
	\begin{frame}{Grafi computazionali e differenziazione automatica}
		\textcolor{red}{C'è un'immediata corrispondenza tra i concetti visti nella prima parte -- quando legati a computational graphs -- e concetti di differenziazione automatica}
		
		\begin{table}[ht]
			\centering
			\caption{\emph{Didascalia}}
			\begin{tabular}{lc}
			\toprule 
				Grafo computazionale & Differenziazione automatica \\
			\midrule
				$w(i,j)$ & Derivata parziale $\pdv{j}{i}$ {\smaller (più precisamente, $\dd i\mapsto \pdv{j}{i}\dd i$)} \\
				Path product del cammino $i\to j$ & Derivata di $j$ rispetto a $i$ \\
				Nodo sorgente & Parametri della funzione \\
				Nodo pozzo & Tipicamente, una \emph{loss function}\\
				\emph{Path weights} & Derivata della \emph{loss} rispetto ai parametri \\
			\bottomrule
			\end{tabular}
		\end{table}
		
		\textcolor{red}{Due parole sulla regola della catena}
	\end{frame}
	
	\subsection{Il grafo computazionale di un MLP}
	\begin{frame}{Grafo computazionale di un MLP}

		\begin{columns}
			\begin{column}{0.4\textwidth}
				\begin{figure}[h]
					\centering
					\includegraphics[width=\textwidth]{Immagini/scalar_MLP_pseudocode}
				\end{figure}
			\end{column}
			\begin{column}{0.6\textwidth}
				\[
					(I-L^{\top}) = 
					\left(\scriptsize\begin{array}{*{3}{c}|*{4}{c}}
						 0 		& \cdots & 0 	  & \delta_1 x_0 & \cdot & \cdot & \cdot \\
						 \cdot  &        & \cdot  & \cdot & \delta_2 x_1 & \cdot & \cdot \\
						 \cdot	&   	 & \cdot  & \cdot & \cdot & \delta_3 x_2 & \cdot \\
						 0 		& \cdots & 0 	  & \cdot & \cdot & \cdot & \delta_4 x_3 \\
						\hline
						 0     & \cdots & 0   	& \cdot & \delta_2 w_2 & \cdot & \cdot \\
						 \cdot & 		& \cdot & \cdot & \cdot & \delta_3 w_3 & \cdot \\
						 \cdot & 		& \cdot & \cdot & \cdot & \cdot & \delta_4 w_4 \\
						 0	   & \cdots & 0 	& \cdot & \cdot & \cdot & \cdot 
					\end{array}\right)
				\]
			\end{column}
		\end{columns}

		\begin{figure}[hp]
			\centering
			\includegraphics[width=0.7\textwidth]{Immagini/MLP_compgraph_edited}
			\caption{\emph{Grafo computazionale di un MLP con tre layer nascosti e un layer di output. Il calcolo della \emph{loss function} non è mostrato.}}
		\end{figure}
		\vspace*{-1em}
		\begin{flalign*}
			\textbf{Idea}:\left\lbrace
			\begin{array}{rll}
				x_i: & \text{unità {\smaller (aka neuroni)} del layer}\ i & \equiv\text{nodo interno {\smaller (indici: $1,\dots,4$)}}\\
				w_i: & \text{parametri {\smaller (aka pesi)} del layer}\ i & \equiv\text{nodo sorgente {\smaller (indici: $5,\dots,8$)}}
			\end{array}\right. &&
		\end{flalign*}
	\end{frame}
	
	\begin{frame}{Grafo computazionale di un MLP}
		Per il MLP scalare, si ha 
		\[
			(I-L^{\top})=
			\left(\begin{matrix}
				I & -M^{\top}\\
				0 & I-\tilde{L}^{\top}
			\end{matrix}\right),\quad
			\text{dove}\quad
			\begin{dcases*}
				M & connessioni tra sorgenti e nodi interni\\
				L & connessioni tra nodi interni
			\end{dcases*}
		\]
		Dunque 
		\[
			(I-L^{\top})^{-1}=
				\left(\begin{matrix}
					I & M^{\top}\!(I-\tilde{L})^{-\!\top}\\
					0 & (I-\tilde{L})^{-\!\top}
				\end{matrix}\right).
		\]
		Di conseguenza, se l'ultimo nodo è l'unico pozzo, si ha
		\[
			\texttt{path\_weight}=M^{\top}(I-\tilde{L}^{\top})^{-1}e_n.
		\]
		
	\end{frame}


\section{Matrici di operatori lineari}	
	\begin{frame}
		{Funzioni lineari tra spazi di matrici}
		\begin{table}[ht]
			\raggedright
			\begin{tabular}{>{\smaller}p{0.12\textwidth}ccc}
			\toprule
				{\larger Nome} & Simbolo & Definizione & Rappresentazione densa \\
			\midrule
				Prodotto di Kronecker di $A$ con $B$ & $A\otimes B$ & {$\begin{array}{rcl}
					\R{m}{n} & \longrightarrow & \R{m_1}{n_1}\\
						X 	& \longmapsto & BX^{\top}A
				\end{array}$}  & $A\otimes B$ {\smaller $(m_1n_1\times mn)$}\\
			\bottomrule
			\end{tabular}
		\end{table}
		
	\end{frame}

	\begin{frame}
		{Funzioni lineari tra spazi di matrici}
		\hrule 
		\begin{columns}
			\begin{column}{0.25\textwidth}
				\centering {\textbf{Nome}}
			\end{column}
			\begin{column}{0.10\textwidth}
				\centering \textbf{Simbolo}
			\end{column}
			\begin{column}{0.30\textwidth}
				\centering \textbf{Definizione}
			\end{column}
			\begin{column}{0.35\textwidth}
				\centering \framebox[1.1\height]{\textbf{Rappresentazione densa}}
				%\vspace{5pt}
				%\centering \textbf{Rappresentazione densa}
				%\vspace{2pt}
			\end{column}
		\end{columns}
		\hrule
	% 1a riga		
		\begin{columns}
			\begin{column}{0.22\textwidth}
				{\smaller Prodotto di Kronecker di $A$ con $B$}
			\end{column}
			\begin{column}{0.08\textwidth}
				\center $A\otimes B$
			\end{column}
			\begin{column}{0.30\textwidth}
				\[\begin{array}{rcl}
					\R{m}{n} & \longrightarrow & \R{m_1}{n_1}\\
						X 	& \longmapsto & BXA^{\top}
				\end{array}\]
			\end{column}
			\begin{column}{0.30\textwidth}
				\center $A\otimes B$ {\smaller $(m_1n_1\times mn)$}
			\end{column}
		\end{columns}
	% 2a riga
		\begin{columns}
			\begin{column}{0.22\textwidth}
				{\smaller Moltiplicazione a sinistra per $B$}
			\end{column}
			\begin{column}{0.08\textwidth}
				\center $B_{\mathrm{L}}$
			\end{column}
			\begin{column}{0.30\textwidth}
				\[\begin{array}{rcl}
					\R{m}{n} & \longrightarrow & \R{m_1}{n}\\
						X 	& \longmapsto & BX
				\end{array}\]
			\end{column}
			\begin{column}{0.30\textwidth}
				\center $I\otimes B$ {\smaller $(m_1n\times mn)$}
			\end{column}
		\end{columns}
	% 3a riga
		\begin{columns}
			\begin{column}{0.22\textwidth}
				{\smaller Moltiplicazione a destra per $A$}
			\end{column}
			\begin{column}{0.08\textwidth}
				\center $A_{\mathrm{R}}$
			\end{column}
			\begin{column}{0.30\textwidth}
				\[\begin{array}{rcl}
					\R{m}{n_1} & \longrightarrow & \R{m}{n}\\
						X 	& \longmapsto & XA
				\end{array}\]
			\end{column}
			\begin{column}{0.30\textwidth}
				\center $A^{\top}\otimes I$ {\smaller $(mn_1\times mn)$}
			\end{column}
		\end{columns}
	% 4a riga
		\begin{columns}
			\begin{column}{0.22\textwidth}
				{\smaller Prodotto di Hadamard con $M$}
			\end{column}
			\begin{column}{0.08\textwidth}
				\center $M_{\mathrm{H}}$
			\end{column}
			\begin{column}{0.30\textwidth}
				\[\begin{array}{rcl}
					\R{m}{n} & \longrightarrow & \R{m}{n}\\
						X 	& \longmapsto & M.*X
				\end{array}\]
			\end{column}
			\begin{column}{0.30\textwidth}
				\center $\operatorname{Diag}(\operatorname{Vec}(M))$ {\smaller $(mn\times mn)$}
			\end{column}
		\end{columns}
	% 5a riga
		\begin{columns}
			\begin{column}{0.22\textwidth}
				{\smaller Prodotto interno matriciale con $G$}
			\end{column}
			\begin{column}{0.08\textwidth}
				\center $G^{\top\cdot}$
			\end{column}
			\begin{column}{0.30\textwidth}
				\[\begin{array}{rcl}
					\R{m}{n} & \longrightarrow & \R\\
						X 	& \longmapsto & \langle G,X\rangle_F
				\end{array}\]
			\end{column}
			\begin{column}{0.30\textwidth}
				\center $\operatorname{Vec}(G)^{\top}$ {\smaller $(1\times mn)$}
			\end{column}
		\end{columns}
		\hrule
	\end{frame}
	
	
	
	
	
	
	
	
    
    \begin{frame}
        \begin{center}
            \Huge{Grazie per l'attenzione!}
        \end{center}
    \end{frame}
    
    \begin{frame}{\refname}
    	\begin{thebibliography}{9}
    		\bibitem{article:BackBackBack} A. Edelman, E. Akyurek, Y. Wang
    		\newblock Backpropagation through Back Substitution with a Backslash
    		\newblock SIAM J. Matrix Anal. Appl. Vol~45 (2024), No.~1, pp.~429-449
    	\end{thebibliography}
    \end{frame}
    
\appendix
\section*{Appendice}
	\begin{frame}
		\begin{center}
			\Huge{\textbf{Appendice}}
		\end{center}
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:path_product_no_top_sort:appendice}{Matrice dei \emph{path products}, senza ordinamento topologico}}
		Sia $L^{\top}\in\R{n}{n}$ la matrice di adiacenza di un DAG $\mathcal{G}$. L'operazione corrispondente alla rietichettatura dei nodi di $\mathcal{G}$ è il coniugio con una matrice di permutazione, dunque $\exists\,P\in\R{n}{n}$ di permutazione tale che $\hat{L}^{\top}\coloneqq P L^{\top} P^{\top}$ è strettamente triangolare superiore. 
		
		\begin{block}{}
			Nella nuova etichettatura, i primi $s$ nodi sono sorgenti e gli ultimi $p$ sono pozzi. Nella vecchia etichettatura, i nodi sorgente sono indicizzati da $P^{\top}e_i$ per $i\in[s]$ {\smaller ($e_i$ è l'$i$-esimo versore canonico)}
		\end{block}
		
		
		Si ha $(I-\hat{L}^{\top})=P(I-L^{\top})P^{\top}$, da cui $(I-\hat{L})^{-\!\top}=P(I-L)^{-\!\top}P^{\top}$.
		Dunque
		\[\text{path weights} =
		\begin{pNiceMatrix}%{cc}
			\Block{2-3}<\large>{I_s} & & \\
			& & \\
			\Hline
			\Block{2-3}<\large>{0} & & \\
			& & \\
		\end{pNiceMatrix}^{\mathllap{\top}}
		\qty(I - \hat{L})^{-\!\top}
		\begin{pNiceMatrix}%{cc}
			\Block{3-2}<\large>{0} & \\
			& \\
			& \\
			\Hline
			\Block{1-2}{I_p} 	   & \\
		\end{pNiceMatrix} =
				\begin{pNiceMatrix}%{cc}
			\Block{2-3}<\large>{I_s} & & \\
			& & \\
			\Hline
			\Block{2-3}<\large>{0} & & \\
			& & \\
		\end{pNiceMatrix}^{\mathllap{\top}}
		P\qty(I - L)^{-\!\top}P^{\top}
		\begin{pNiceMatrix}%{cc}
			\Block{3-2}<\large>{0} & \\
			& \\
			& \\
			\Hline
			\Block{1-2}{I_p} 	   & \\
		\end{pNiceMatrix}
		\]
		
		Ci siamo ricondotti al caso precedente. I path weights sono dati dagli elementi di $(I-L)^{-\!\top}$ di indici dati dai nodi sorgente/pozzo secondo l'enumerazione originale.
		
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:path_product_no_top_sort}{\faHandPointLeft}}
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:backsub_is_backprop:appendice}{Backsubstitution $\equiv$ backpropagation $\qquad 1/2$}}
		{Formule della sostituzione all'indietro}
		
		Ricordiamo che la risoluzione $Tx=b$ tramite sostituzione all'indietro è data da
		\[
		\begin{dcases}
			x_n = b_n / T_{n,n} & \\
			x_k = \frac{1}{T_{k,k}}\smashoperator[r]{\sum_{j=k+1}^n} T_{k,j}x_k & k=n-1,\dots,1
		\end{dcases}
		\]
		
		Supponiamo $\mathcal{G}$ topologicamente ordinato. Allora la matrice di adiacenza $L^{\top}\in\R{n}{n}$ è strettamente triangolare superiore. Consideriamo per semplicità un singolo pozzo, rappresentato da $e_n$. La risoluzione di $(I-L)^{\top}x=e_n$ per sostituzione all'indietro diventa
		\begin{equation}\label{eqn:backsub_ImLt}
		\begin{dcases}
			x_n = 1/1 = 1& \\
			x_k = \smashoperator[lr]{\sum_{j=k+1}^n} (I-L^{\top})_{k,j}\,x_k = \smashoperator[lr]{\sum_{j=k+1}^n} (L^{\top})_{k,j}\,x_j
		\end{dcases}	
		\end{equation}
	
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:backsub_is_backprop}{\faHandPointLeft}}
	\end{frame}
				
	\begin{frame}
		{Backsubstitution $\equiv$ backpropagation $\qquad 2/2$}
		{Dimostrazione}
		
		\vspace{-1em}
		\begin{proposizione}
			$x_k$ restituito dalla~\eqref{eqn:backsub_ImLt} accumula {\smaller ($\equiv$ somma)} tutti i path product da $k$ a $n$.
		\end{proposizione}	
		\begin{block}{Dimostrazione.}
			Per induzione su $k=n,\dots,1$.
			\begin{itemize}
				\item \textbf{Passo base}: $x_n=1$ è banalmente vero
				\item \textbf{Passo induttivo}: consideriamo $\hat{\jmath}>k$. Poiché $(L^{\top})_{k,\hat{\jmath}}$ è il peso dell'arco $k\to\hat{\jmath}$ e $x_{\hat{\jmath}}$ è --per ipotesi induttiva -- il path product del cammino $\hat{\jmath}\to n$, 
				$(L^{\top})_{k,\hat{\jmath}}\,x_{\hat{\jmath}}$ è il path product del cammino da $k$ a $n$ che passa da $\hat{\jmath}$ come primo passo.
				\smallskip 
				
				Dunque ciascun addendo di~\eqref{eqn:backsub_ImLt} è il path product di un cammino da $k$ a $n$.
				\smallskip 
				
				Ne segue che $x_k = \sum_{j=1}^n (L^{\top})_{k,j}\,x_j = \sum_{j=k+1}^n (L^{\top})_{k,j}\,x_j$, poiché $L^{\top}$ è triangolare superiore. \qedhere
			\end{itemize}
		\end{block}
		
		Di conseguenza, risolvere $(I-L)^{\top}x=e_n$ tramite sostituzione all'indietro corrisponde a calcolare i path weights visitando $\mathcal{G}$ dal pozzo alle sorgenti.
		
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:backsub_is_backprop}{\faHandPointLeft}}
	\end{frame}
	
	
	\begin{frame}
		{\hypertarget{frame:different_orders:appendice}{Strategie alternative per valutare \texttt{path\_weights}}}
		{Ciascuna strategia di valutazione corrisponde a un diverso ordine di visita}
		
		$I-L^{\top}$ è triangolare superiore, e ha $1$ sulla diagonale. \`E possibile scrivere $(I-L)^{\top}$ come prodotto di matrici elementari di Gauss. Ciò corrisponde a fare eliminazione gaussiana (per colonne) per ridurre $I-L^{\top}$ all'identità, e poi invertire: $I-L^{\top}=E_1\cdot\ldots\cdot E_m$ dove ciascuna $E_i$ è una matrice elementare di Gauss.
		Dunque 
		\[
		\texttt{path\_weights}=
		\left\lgroup\begin{matrix}
			I_s \\ 
			\hline
			0
		\end{matrix}\right\rgroup^{\mathclap{\top}}
		(I-L)^{-\!\top}
		\left\lgroup\begin{matrix}
			0 \\ 
			\hline
			I_p
		\end{matrix}\right\rgroup = 
		\left\lgroup\begin{matrix}
			I_s \\ 
			\hline
			0
		\end{matrix}\right\rgroup^{\mathclap{\top}}
		E_m^{-1}\cdot\ldots\cdot E_1^{-1}
		\left\lgroup\begin{matrix}
			0 \\ 
			\hline
			I_p
		\end{matrix}\right\rgroup.		
		\]
		
		La scelta di un ``inserimento di parentesi'' restituisce un ordine nel calcolo dei path products. Moltiplicare due matrici elementari corrisponde a lavorare su sottografi {\smaller (es. calcolare path weights in \emph{reverse mode} su un sottografo)}; ossia a calcolare path products ``parziali'' che poi vengono utilizzati per calcolare path products di cammini che hanno quello già considerato come sottocammino.
		
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:backsub_is_backprop}{\faHandPointLeft}}
	\end{frame}

\end{document}

