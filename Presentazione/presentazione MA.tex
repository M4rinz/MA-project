\documentclass[10pt,xcolor={table,dvipsnames}]{beamer} 		% carica automaticamente amsthm, amssymb, amsmath, graphicx

\usepackage{appendixnumberbeamer}		% Per numerare le slides in Beamer sì da non contare quelle di appendice
\usepackage[T1]{fontenc}				% codifica dei font
\usepackage[utf8]{inputenc}				% lettere accentate da tastiera
\usepackage[italian]{babel}				% lingua del documento
\usepackage[italian]{varioref}			% Per usare il comando \vref{label}, che dà dei collegamenti più dettagliati

% Load the custom style file
\usepackage{AndreaStyle}
% The file `AndreaStyle.sty` is stored in: `D:\Programmi e Applicazioni\texlive\texmf-local\tex\latex\local` for Windows.
% The file `AndreaStyle.sty` is stored in: `/usr/local/texlive/texmf-local/tex/latex/local` for Ubuntu (desktop).
% This won't work in Overleaf, until the AndreaStyle.sty file is added to the project

%\usepackage{braket} 					% Per il comando \Set, e altre (poche) cose
%\usepackage{textcomp}					% Dovrebbe aggiungere più simboli

\usepackage{algorithm}                  % Definisce l'ambiente algorithm (ha numero e didascalia)
\usepackage[beginLComment=//~,endLComment=~]{algpseudocodex} % Per scrivere pseudocodice

\usepackage{relsize}					% Per usare \mathbigger{} ecc

\usepackage{multirow}					% per usare il comando multirow
\usepackage{tabularx}					% per fare tabelle. Carica il pacchetto array, per gli array.
\usepackage{arydshln}					% per le linee tratteggiate nelle tabelle

\usepackage{nicematrix}

%\usepackage[rightcaption]{sidecap}		% Per mettere le didascalie di lato

%\usepackage{tikz}
\usepackage{fontawesome5}				% Aggiunge simboli da FontAwesome

\usepackage[many]{tcolorbox}
\usepackage{hyperref}					% Importante: hyperref va caricato nel documento.



%\setcounter{tocdepth}{1}	% profondità dell'indice

	% ------ TEOREMI CUSTOM ------
\theoremstyle{plain}					% Definisce ambienti per Teoremi, esercizi, corollari... Con lo stile adeguato
	\newtheorem{proposizione}{Proposizione}[section]
	\newtheorem*{proposizione*}{Proposizione}
	
	\newtheorem{teorema}{Teorema}[section]
	\newtheorem*{teorema*}{Teorema}
		
	%\newtheorem{lemma_es}{Lemma}[esercizio]
	%\newtheorem{lemma}{Lemma}[section]
	\newtheorem*{lemma*}{Lemma}
	\newtheorem{corollario}{Corollario}[section]


\theoremstyle{definition}				
	\newtheorem{definizione}{Definizione}[section]%[chapter]
	\newtheorem*{definizione*}{Definizione}	%definizione non numerata
	\newtheorem*{notazione}{Notazione}

\theoremstyle{remark}
	\newtheorem{oss}{Osservazione}[section]
	\newtheorem*{oss*}{Osservazione}
	
	% ------ COMANDI CUSTOM ------

% Define a command to create unnumbered footnotes
\let\svthefootnote\thefootnote
\textheight 1in
\newcommand\blankfootnote[1]{%
	\let\thefootnote\relax\footnotetext{#1}%
	\let\thefootnote\svthefootnote%
}

% Rinomina i "Require" ed "Ensure" dei listings creati con algorithmicx
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}


	% ------ COLORI CUSTOM ------

	
	
% TCOLORBOX 
\newtcolorbox{notitleblock}{
	enhanced,
	size = fbox,
	outer arc = 0mm,
	arc = 0mm,
	oversize, 
	colback = UniWhite, 
	frame hidden, 
	coltext = DMDarkBlue
}

	% ------- ALTRA BUROCRAZIA -------
\setbeamertemplate{page number in head/foot}[appendixframenumber] % Per non numerare le pagine in appendice
%\setbeamertemplate{theorems}[numbered]		% Per numerare i teoremi etc nella presentazione (una cosa che di solito non si fa)

% ========================================= INIZIO CODICE =========================================
	% Tema alternativo (Richiede XeLaTeX)
%\usetheme{SNSPisa}
%\setbeamercolor{blockcolor}{bg=UniWhite, fg=DMDarkBlue}
    % Tema solito 
\usetheme{Madrid}

\title[Seminario MA]{Backpropagation through Back substitution with a Backslash}
%\subtitle{Presentazione e dimostrazione della convergenza} 
\author{Andrea Marino}
\institute[DI UniPi]{Università di Pisa}
%\titlegraphic{\includegraphics[width=2cm]{Immagini/cherubino_black.eps}}
\date[\today]{Metodi di Approssimazione\newline Seminario di fine corso}

% --------------------- COME MOSTRARE APPENDICE --------------------
% ----------------------------- start ------------------------------
	% The sections begin with a table of contents slide in which all sections are shaded, but the 
	%	current section is visible
	% Each subsection begins with a table of contents slides in which the current section is shown, the 
	% 	current subsection is visible and the other subsections within the section are shaded
\AtBeginSection[] 					
{
	\begin{frame}
		\frametitle{Sommario}
		\tableofcontents[
		currentsection, % Only show the current section
		subsectionstyle=show/hide/hide % Show subsections of the current section, hide everything else
		] 
	\end{frame}
}

\AtBeginSubsection[]
{
	\begin{frame}
		\frametitle{Sommario}
		\tableofcontents[
		currentsection, % Only show the current section
		subsectionstyle=show/shaded/hide % Show the current subsection, shade others in the same section, hide everything else
		]
	\end{frame}
}
% ----------------------------- end ------------------------------



\begin{document}
	\begin{frame}[plain]
		\titlepage
	\end{frame}
	
	% Slide di sommario iniziale (La prima sezione inizia con un nuovo sommario, quindi decommentando il frame
	%	qui sotto ci saranno due slides di sommario, ma questa mostrerà tutte le sezioni (non le sottosezioni))
%\section*{Sommario}
%	\setcounter{tocdepth}{1}
%	\begin{frame}
%		\frametitle{Sommario}
%		\tableofcontents
%	\end{frame}	
%	\setcounter{tocdepth}{2}

%\section{Introduzione}
	\begin{frame}{Introduzione}
		\textbf{Obiettivo}: implementare la backpropagation, in Julia, usando il backslash.
		\medskip
		
		\onslide<2->{Descriveremo la \emph{backpropagation} usando il linguaggio dell'algebra lineare.}
		\onslide<3->{Vedremo come ci sia una corrispondenza tra:}
		\begin{itemize}
			\item<4-> Calcolo delle derivate di una funzione
			\item<5-> Calcolo di specifiche quantità sul suo \emph{grafo computazionale}
			\item<6-> Operazioni tra particolari matrici.
		\end{itemize} 
		\medskip
		
		\onslide<7->{Introducendo un tipo di dato per rappresentare funzioni lineari, dopo un minimo di setup per definire delle elementari operazioni su questo tipo di dato {\smaller (ma senza overloading del backslash!)}, il calcolo di $\grad{J}$ sarà dato da 
		\[
			\grad{J} = M^{\top}\qty((I - \tilde{L}) \,\backslash\, g)
		\]
		dove $M$ e $\tilde{L}$ sono matrici che hanno operatori come elementi.}
		
%		\onslide<3->{In particolare:}
%		\begin{enumerate}
%			\item<3-> Introdurremo i \emph{path weights} su un DAG, descrivendo il loro calcolo con il linguaggio dell'algebra lineare
%			\item<4-> Tracceremo un collegamento tra entità e operazioni su \emph{computational graph} e concetti di automatic differentiation {\smaller (in particolare, \emph{path weights} $\equiv$ derivate totali)}
%			\item<5-> Sarà possibile definire matrici di operatori {\smaller (aka il tipo di dato)}
%			\item<6-> Risoluzione di un sistema lineare $\equiv$ calcolo di tutte le derivate totali
%		\end{enumerate}		
	\end{frame}
    
    
\section{Cammini, \emph{path weights} su DAG}
	\subsection{Definizioni e prime proprietà}
	\begin{frame}{Definizioni $\qquad 1/2$}{\emph{Path products} e \emph{path weights}}
		Consideriamo un DAG {\smaller (connesso!)} $\mathcal{G}$ e un cammino {\smaller (orientato!)} su $\mathcal{G}$.
		
		\begin{definizione}<2->
			Il \alert{path product} di un cammino su $\mathcal{G}$ è dato dal prodotto dei pesi degli archi che lo costituiscono
		\end{definizione}
		
		\onslide<3->{Assumiamo che in $\mathcal{G}$ non vi siano self loops. D'ufficio, il path product da un qualsiasi nodo a sé stesso è $1$.}
		
		\begin{definizione}<4->
			Un \alert{path weight} su $\mathcal{G}$ è il path product di un cammino da un nodo sorgente a un nodo pozzo
		\end{definizione}
		\medskip	
	
		\begin{oss}<5->
			Il ``prodotto'' è da intendersi nel senso di teoria dei gruppi. 
		\end{oss}

	\end{frame}
	
	\begin{frame}{Definizioni $\qquad 2/2$}{Matrice di adiacenza}
		\textbf{Abuso di notazione}: un nodo viene identificato con la sua etichetta $i\in[n]$%\coloneqq\qty{1,\dots,n}$.
		
		\begin{definizione}
			La \alert{matrice di adiacenza} $L$ di $\mathcal{G}$ è data da:
			\[
				\R{n}{n}\ni (L^{\top})_{i,j}= 
				\begin{dcases*}
					0 & {\smaller Non esiste un arco $i\to j$}\\
					w(i,j) & {\smaller Esiste un arco $i\to j$, con peso $w(i,j)$}
				\end{dcases*}
			\]
		\end{definizione}
		\medskip 
		
		\onslide<2->{Ricordiamo che $(L^{\top})^k$ accumula {\smaller ($\equiv$ somma)} i path products di lunghezza $k$:}
		\onslide<3->{$(L^{\top})^k_{i,j}$ contiene la somma dei pesi di tutti i cammini da $i$ a $j$ di lunghezza $k$.}
	
		
		\blankfootnote{$[n]\coloneqq\qty{1,\dots, n}$}
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:path_product_no_top_sort}{Matrice di adiacenza e \emph{path products}}}
		Se i nodi di $\mathcal{G}$ sono enumerati seguendo un ordinamento topologico, $L^{\top}$ è strettamente triangolare superiore, dunque nilpotente. \onslide<2->{Di conseguenza 
		\[
			(I-L^{\top})^{-1}=I + L^{\top} + \dots + (L^{\top})^{n-1}.
		\]}
		%\vspace*{-1em}
		\onslide<3->{\noindent Dunque $(I-L)^{-\!\top}$ contiene tutti i path products massimali, compresi i path weights.}
		\medskip
		
		\onslide<4->{Se i nodi sorgente sono $1,\dots,s$ e i nodi pozzo $n-p+1,\dots,n$, si ha
		\begin{equation*}\label{eqn:path_weights}
			\texttt{path\_weights}=
			\underbrace{
				\left\lgroup\begin{array}{cc}
					\multicolumn{2}{c}{\multirow{2}*{$I_s$}}\\
					& \\
					\hline
					\multicolumn{2}{c}{\multirow{2}*{$\mathlarger{0}$}} \\
					& \\
				\end{array}\right\rgroup^{\mathllap{\top}}
			}_{\text{sorgenti}}
			\!\qty(I - L)^{-\!\top}\!\!
			\underbrace{
				\left\lgroup\begin{array}{cc}
					\multicolumn{2}{c}{\multirow{3}*{$\mathlarger{0}$}} \\
					& \\
					& \\
					\hline
					\multicolumn{2}{c}{I_p}\\
				\end{array}\right\rgroup
			}_{\text{pozzi}}
		\end{equation*}}
		\vspace*{-1em}
		\begin{oss}<5->
			Il metodo è valido anche senza ordinamento topologico, basta selezionare le righe/colonne corrispondenti alle sorgenti/pozzi. (cfr. Appendice~\hyperlink{frame:path_product_no_top_sort:appendice}{\faHandPointRight})
		\end{oss}
	\end{frame}
	
	% Versione della formula con NiceMatrix (più lenta)
%	\begin{frame}{title}
%		\[
%		\text{path weights}=
%		\begin{pNiceMatrix}%{cc}
%			\Block{2-3}<\large>{I_s} & & \\
%			& & \\
%			\Hline
%			\Block{2-3}<\large>{0} & & \\
%			& & \\
%			\CodeAfter
%			\UnderBrace[yshift=2pt]{4-1}{4-3}{\text{\smaller sorgenti}}
%		\end{pNiceMatrix}^{\mathllap{\top}}
%		\qty(I - L)^{-\!\top}
%		\begin{pNiceMatrix}%{cc}
%			\Block{3-2}<\large>{0} & \\
%			& \\
%			& \\
%			\Hline
%			\Block{1-2}{I_p} 	   & \\
%			\CodeAfter
%			\UnderBrace[yshift=2pt]{5-1}{5-2}{\text{\smaller pozzi}}
%		\end{pNiceMatrix}
%		\]
%	\end{frame}


	\begin{frame}{Esempio: DAG completo con quattro nodi}
		\begin{columns}
			\begin{column}{0.7\textwidth}
				\begin{figure}[ht]
					\centering
					\includegraphics[width=\textwidth]{Immagini/DAG_completo}
					\caption{\emph{Grafo, \emph{path weights}, matrice di incidenza e matrice dei \emph{path products}.} {\smaller (Immagine tratta dall'articolo originale)}}
				\end{figure}
			\end{column}
			\begin{column}{0.3\textwidth}
				\begin{align*}
					& \begin{multlined}
						(L^{\top})^2 = \\
						= \!\left(\begin{smallmatrix}
							\cdot & \cdot & a_1b_2 & a_1c_2+b_1c_3\\
							& \cdot & \cdot  & b_2c_3 \\
							& 	   & \cdot  & \cdot \\
							& 	   & 	    & \cdot 
						\end{smallmatrix}\right)
					\end{multlined} \\
					& (L^{\top})^3 = 
						\left(\begin{smallmatrix}
							\cdot & \cdot & \cdot & a_1b_2c_3\\
								  & \cdot & \cdot & \cdot \\
								  & 	  & \cdot & \cdot \\
								  & 	  & 	  & \cdot 
						\end{smallmatrix}\right)\\
				\end{align*}
			\end{column}
		\end{columns}
		
	\end{frame}

	\subsection{Calcolo dei \emph{path weights}}
	\begin{frame}
		{\hypertarget{frame:backsub_is_backprop}{Backpropagation come sostituzione all'indietro}}
		
		Quanto appena visto mostra che si può formulare il problema del calcolo dei \emph{path weights} in termini di algebra lineare.
		\smallskip
		
		\onslide<2->{Per l'associatività del prodotto, esistono diversi modi di calcolare i \emph{path weights}: dalle sorgenti ai pozzi {\smaller (\emph{forward})}, dai pozzi alle sorgenti {\smaller (\emph{backward})}.}
		
		\begin{block}<3->{}
			In generale, diversi modi di valutare l'espressione $\texttt{path\_weights}=\left(\begin{smallmatrix}I_s \\ 0\end{smallmatrix}\right)^{\!\!\top}(I-L)^{-\!\top}\left(\begin{smallmatrix}0 \\ I_p\end{smallmatrix}\right)$ corrispondono a diverse ``strategie'' per calcolare i \emph{path products}. (cfr. Appendice~\hyperlink{frame:different_orders:appendice}{\faHandPointRight}) 			
			\smallskip 
			
			\textbf{Idea}: a ciascun modo di ``inserire parentesi'' corrisponde un ordine di visita di $\mathcal{G}$. 
		\end{block}
		
		\onslide<4->{In particolare, calcolare \texttt{path\_weights} risolvendo $(I-L^{\top})x=\left(\begin{smallmatrix}0 \\ I_p\end{smallmatrix}\right)$ tramite sostituzione all'indietro corrisponde a un calcolo che procede dai pozzi alle sorgenti (cfr. Appendice~\hyperlink{frame:backsub_is_backprop:appendice}{\faHandPointRight}).}
		\smallskip 
		
		\onslide<5->{Questa strategia corrisponde alla backpropagation per calcolare path products su grafi computazionali.}
	\end{frame}
	

\section{Grafi computazionali}
	\subsection{Definizione, relazioni tra DAG e AD}
	\begin{frame}{Grafi computazionali}
		Un \emph{grafo computazionale} rappresenta la ``struttura'' di una funzione. Mostrando quali \emph{step} sono richiesti per la valutazione, un grafo computazionale può descrivere un algoritmo di calcolo.
		
		\begin{definizione}<2->
			Un \alert{grafo computazionale} è un DAG etichettato, in cui:
			\begin{itemize}
				\item i \textbf{nodi sorgente} sono etichettati con nomi di variabili
				\item i \textbf{nodi interni}, oltre a nomi di variabili, contengono formule che dipendono dai nodi entranti
				\item ciascun \textbf{arco}  $i\to j$ è etichettato con $\pdv*{j}{i}$ {\smaller (ovvero la derivata della formula del nodo $j$ rispetto alla variabile $i$)}
			\end{itemize}
		\end{definizione}
		
		\begin{oss}<3->
			Consideriamo grafi computazionali che derivano da algoritmi ``semplici''. {\smaller (no funzioni $\notin C^1$, niente $\mathtt{rand}()$, no \texttt{if\textvisiblespace else\textvisiblespace} risolvibili solo a runtime, \dots)}
		\end{oss}
		
	\end{frame}
	
	\begin{frame}{Grafi computazionali e differenziazione automatica}{}	
		\vspace*{-1.5em}
		{\renewcommand{\arraystretch}{1.05}
		\begin{table}[ht]
			\centering
			\caption{\emph{Corrispondenza tra entità dei grafi computazionali e concetti di AD}}
			\begin{tabular}{>{\centering\arraybackslash}m{0.3\textwidth}>{\centering\arraybackslash}m{0.6\textwidth}}
			\toprule 
				\textbf{Grafo computazionale} & \textbf{Differenziazione automatica} \\
			\midrule
				$w(i,j)$ & Derivata parziale $\pdv{j}{i}$ {\smaller (più precisamente, $\dd i\mapsto \pdv{j}{i}\dd i$)} \\
				Path product \newline del cammino $i\to j$ & Derivata di $j$ rispetto a $i$ \\
				Nodo sorgente & Parametri della funzione \\
				Nodo pozzo & Tipicamente, una \emph{loss function}\\
				\emph{Path weights} & Derivata della \emph{loss} rispetto ai parametri \\
			\bottomrule
			\end{tabular}
		\end{table}}
		
		\onslide<2->{\textbf{Regola della catena}: ``il differenziale della composizione è la composizione dei differenziali''.} 
		
		\onslide<2->{In coordinate: ``il Jacobiano della composizione è dato dal prodotto dei Jacobiani''.}
	%	\smallskip 
			
		\begin{block}<3->{}
			Nel contesto dei grafi computazionali, calcolo dei \emph{path weights} $\equiv$ calcolo della derivata secondo la regola della catena. \onslide<4->{Precisamente, 
			per calcolare la derivata rispetto a un input bisogna accumulare tutti i \emph{path weights} dalla sorgente al pozzo corrispondenti.}
		\end{block}
	\end{frame}
	
	\subsection{Il grafo computazionale di un MLP}
	\begin{frame}{Grafo computazionale di un MLP$\qquad 1/2$}{Algoritmo e rappresentazione grafica}

		\begin{columns}
			\begin{column}{0.4\textwidth}
				\begin{figure}[h]
					\centering
					\includegraphics[width=\textwidth]{Immagini/scalar_MLP_pseudocode}
				\end{figure}
			\end{column}
			\begin{column}{0.6\textwidth}
				\[
					L^{\top} = 
					\left(\scriptsize\begin{array}{*{3}{c}|*{4}{c}}
						 0 		& \cdots & 0 	  & \delta_1 x_0 & \cdot & \cdot & \cdot \\
						 \cdot  &        & \cdot  & \cdot & \delta_2 x_1 & \cdot & \cdot \\
						 \cdot	&   	 & \cdot  & \cdot & \cdot & \delta_3 x_2 & \cdot \\
						 0 		& \cdots & 0 	  & \cdot & \cdot & \cdot & \delta_4 x_3 \\
						\hline
						 0     & \cdots & 0   	& \cdot & \delta_2 w_2 & \cdot & \cdot \\
						 \cdot & 		& \cdot & \cdot & \cdot & \delta_3 w_3 & \cdot \\
						 \cdot & 		& \cdot & \cdot & \cdot & \cdot & \delta_4 w_4 \\
						 0	   & \cdots & 0 	& \cdot & \cdot & \cdot & \cdot 
					\end{array}\right)
				\]
			\end{column}
		\end{columns}

		\begin{figure}[hp]
			\centering
			\includegraphics[width=0.7\textwidth]{Immagini/MLP_compgraph_edited}
			\caption{\emph{Grafo computazionale di un MLP con tre layer nascosti e un layer di output. Il calcolo della \emph{loss function} non è mostrato.}}
		\end{figure}
		\vspace*{-1em}
		\begin{flalign*}
			\textbf{Idea}:\left\lbrace
			\begin{array}{rll}
				x_i: & \text{unità {\smaller (aka neuroni)} del layer}\ i & \equiv\text{nodo interno {\smaller (indici: $1,\dots,4$)}}\\
				w_i: & \text{parametri {\smaller (aka pesi)} del layer}\ i & \equiv\text{nodo sorgente {\smaller (indici: $5,\dots,8$)}}
			\end{array}\right. &&
		\end{flalign*}
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:adding_loss}{Grafo computazionale di un MLP$\qquad 2/2$}}
		{Calcolo dei path weights}
		Per il MLP scalare, si ha 
		\[
			(I-L^{\top})=
			\left(\begin{matrix}
				I & -M^{\top}\\
				0 & I-\tilde{L}^{\top}
			\end{matrix}\right),\quad
			\text{dove}\quad
			\begin{dcases*}
				M & connessioni tra sorgenti e nodi interni\\
				\tilde{L} & connessioni tra nodi interni
			\end{dcases*}
		\]
		\onslide<2->{Dunque 
		\[
			(I-L^{\top})^{-1}=
				\left(\begin{matrix}
					I & M^{\top}\!(I-\tilde{L})^{-\!\top}\\
					0 & (I-\tilde{L})^{-\!\top}
				\end{matrix}\right).
		\]}
		\onslide<3->{Di conseguenza, se l'ultimo nodo è l'unico pozzo, si ha
		\[
			\texttt{path\_weights}=M^{\top}(I-\tilde{L}^{\top})^{-1}e_n.
		\]}
		\vspace*{-1em}
		
		\begin{block}<4->{}
			Considerare la loss $L$ corrisponde ad aggiungere un nodo $L$ e un arco $x_N\to L$, con peso $\delta_L\coloneqq L'(x_N)$. {\smaller ($L$ è il nuovo pozzo)}
			
			\onslide<5->{\emph{Turns out} che, in \texttt{path\_weights}, compare $(0,\dots,0,\delta_L)^{\top}$ al posto di $e_n$ (cfr. Appendice~\hyperlink{frame:adding_loss:appendice}{\faHandPointRight}).} \onslide<6->{Le dimensioni restano immutate. Questo è rappresentativo del caso generale.}
		\end{block}
	\end{frame}


\section{Matrici di operatori lineari}	
	\subsection{Alcune funzioni lineari tra spazi di matrici}
	\begin{frame}
		{\hypertarget{frame:adjoints}{Funzioni lineari tra spazi di matrici}}
		
		Siano $A\in\R{n_1}{n}, B\in\R{m_1}{m}, M\in\R{m}{n}, G\in\R{m}{n}$
		\smallskip
		\hrule 
		\begin{columns}
			\begin{column}{0.25\textwidth}
				\centering {\textbf{Nome}}
			\end{column}
			\begin{column}{0.10\textwidth}
				\centering \textbf{Simbolo}
			\end{column}
			\begin{column}{0.30\textwidth}
				\centering \textbf{Definizione}
			\end{column}
			\begin{column}{0.35\textwidth}
				\parbox[c][2.8ex][c]{\linewidth}{\centering\textbf{Rappresentazione densa}}
			\end{column}
		\end{columns}
		\hrule
	% 1a riga		
		\begin{columns}
			\begin{column}{0.22\textwidth}
				{\smaller Prodotto di Kronecker di $A$ con $B$}
			\end{column}
			\begin{column}{0.08\textwidth}
				\center $A\otimes B$
			\end{column}
			\begin{column}{0.30\textwidth}
				\[\begin{array}{rcl}
					\R{m}{n} & \longrightarrow & \R{m_1}{n_1}\\
						X 	& \longmapsto & BXA^{\top}
				\end{array}\]
			\end{column}
			\begin{column}{0.30\textwidth}
				\center $A\otimes B$ {\smaller $(m_1n_1\times mn)$}
			\end{column}
		\end{columns}
	% 2a riga
		\begin{columns}
			\begin{column}{0.22\textwidth}
				{\smaller Moltiplicazione a sinistra per $B$}
			\end{column}
			\begin{column}{0.08\textwidth}
				\center $B_{\mathrm{L}}$
			\end{column}
			\begin{column}{0.30\textwidth}
				\[\begin{array}{rcl}
					\R{m}{n} & \longrightarrow & \R{m_1}{n}\\
						X 	& \longmapsto & BX
				\end{array}\]
			\end{column}
			\begin{column}{0.30\textwidth}
				\center $I\otimes B$ {\smaller $(m_1n\times mn)$}
			\end{column}
		\end{columns}
	% 3a riga
		\begin{columns}
			\begin{column}{0.22\textwidth}
				{\smaller Moltiplicazione a destra per $A$}
			\end{column}
			\begin{column}{0.08\textwidth}
				\center $A_{\mathrm{R}}$
			\end{column}
			\begin{column}{0.30\textwidth}
				\[\begin{array}{rcl}
					\R{m}{n_1} & \longrightarrow & \R{m}{n}\\
						X 	& \longmapsto & XA
				\end{array}\]
			\end{column}
			\begin{column}{0.30\textwidth}
				\center $A^{\top}\otimes I$ {\smaller $(mn_1\times mn)$}
			\end{column}
		\end{columns}
	% 4a riga
		\begin{columns}
			\begin{column}{0.22\textwidth}
				{\smaller Prodotto di Hadamard con $M$}
			\end{column}
			\begin{column}{0.08\textwidth}
				\center $M_{\mathrm{H}}$
			\end{column}
			\begin{column}{0.30\textwidth}
				\[\begin{array}{rcl}
					\R{m}{n} & \longrightarrow & \R{m}{n}\\
						X 	& \longmapsto & M\,.\!*X
				\end{array}\]
			\end{column}
			\begin{column}{0.30\textwidth}
				\center $\operatorname{Diag}(\operatorname{Vec}(M))$ {\smaller $(mn\times mn)$}
			\end{column}
		\end{columns}
	% 5a riga
		\begin{columns}
			\begin{column}{0.22\textwidth}
				{\smaller Prodotto interno matriciale con $G$}
			\end{column}
			\begin{column}{0.08\textwidth}
				\center $G^{\top\bullet}$
			\end{column}
			\begin{column}{0.30\textwidth}
				\[\begin{array}{rcl}
					\R{m}{n} & \longrightarrow & \R\\
						X 	& \longmapsto & \langle G,X\rangle_F
				\end{array}\]
			\end{column}
			\begin{column}{0.30\textwidth}
				\center $\operatorname{Vec}(G)^{\top}$ {\smaller $(1\times mn)$}
			\end{column}
		\end{columns}
		\hrule
		\smallskip 
		
		Dove $\langle G,X\rangle_F\coloneqq\tr(G^{\top}X)=\sum_{i,j}G_{i,j}X_{i,j}.\qquad$
		{\smaller (Per le aggiunte, cfr. Appendice~\hyperlink{frame:adjoints:appendice}{\faHandPointRight})}
	\end{frame}
	
	\begin{frame}{L'operatore $\fbox{\rule{0pt}{0.45em}\rule{0.3em}{0pt}}^{\top\bullet}$}
		\begin{notazione}
			Per un'operatore lineare $\mathcal{L}$, $\mathcal{L}\,X$ può denotare
			\begin{itemize}
				\item l'\textbf{applicazione} di $\mathcal{L}$ a $X$, $\mathcal{L}(X)$ {\smaller (se $X$ è un punto)}
				\item la \textbf{composizione} di $\mathcal{L}$ con $X$, $\mathcal{L}\circ X$ {\smaller (se $X$ è un omomorfismo)}
			\end{itemize} 
		\end{notazione}
		\begin{notazione}<2->
			A seconda del contesto, $\colorbox{gray!30}{\phantom{$\cdot$}}^{\top}\!$ denota la matrice trasposta, o l'operatore aggiunto.
		\end{notazione}
		
		\begin{lemma}<3->
			Sia $\mathcal{L}\colon\R{p}{q}\to\R{m}{n}$ lineare, e $G\in\R{m}{n}$. Si ha
			$\qty(\mathcal{L}^{\top}\,G)^{\top\bullet}=G^{\top\bullet}\,\mathcal{L}$
		\end{lemma}
		\begin{proof}<4->
			Per ogni $X\in\R{p}{q}$,
			$
				\qty(\mathcal{L}^{\top}\,G)^{\top\bullet}\,X = \langle\mathcal{L}^{\top}\,G,X\rangle_F=
				\langle G,\mathcal{L}\,X\rangle_F = \qty(G^{\top\bullet}\,\mathcal{L})\,X
			$
		\end{proof}
	\end{frame}
	
	\subsection{Il MLP nel caso generale}
	\begin{frame}{MLP, caso generale $\qquad 1/2$}{Algoritmo e rappresentazione grafica}
		\begin{columns}
			\begin{column}{0.5\textwidth}
				\begin{figure}[h]
					\centering
					\includegraphics[width=\textwidth]{Immagini/matrix_MLP_pseudocode}
				\end{figure}
			\end{column}
			\begin{column}{0.5\textwidth}
				Archi:
				\begin{align*}
					X_{i-1} & \xlongrightarrow{\pdv*{X_i}{X_{i-1}}}X_i\\
					W_i	    & \xlongrightarrow{\pdv*{X_i}{W_i}}X_i\\
					b_{i}   & \xlongrightarrow{\pdv*{X_i}{b_i}}X_i
				\end{align*}
			\end{column}
		\end{columns}
		\begin{figure}[hp]
			\centering
			\includegraphics[width=0.7\textwidth]{Immagini/general_MLP_compgraph_edited}
			\caption{\emph{Grafo computazionale di un MLP con tre layer nascosti, un layer di output e il calcolo della loss function.}}
		\end{figure}
		\vspace*{-1em}		
	%	Per $A\in\R{m}{n}$, $h\colon\R\to\R$, $\R{m}{n}\ni h.(A)\coloneqq\qty(h(a_{i,j}))_{i,j}$ {\smaller (applicazione \emph{pointwise} di $h$)}
		Vogliamo scrivere la matrice di adiacenza del grafo computazionale
	\end{frame}
	
	\begin{frame}{MLP, caso generale $\qquad 2/2$}{Aggiunta di \emph{loss} e bias}
		\textcolor{red}{veloci remarks su aggiunta di loss e bias}
		
		\textcolor{red}{Un recap veloce dell'ML, e inquadrare quello che è il nostro problema (scrivere la derivata totale della loss. Vogliamo farlo a modo nostro, usando la \texttt{machinery} che abbiamo predisposto (ossia calcolare path weights). Dunque ci serve la matrice di adiacenda del grafo computazionale del MLP)}
		
		\textcolor{red}{Boh forse dire che quando si fa il forward pass abbiamo $X_N$ (è una matrice), e quindi possiamo calcolare $\grad_{X_n}{L}$. Però vogliamo calcolare la derivata totale della loss.}
		
		\textcolor{red}{Devo pensare a come si ricollega al fatto di avere una matrice di operatori, e un vettore (termine noto del sistema) che ha come elementi lo zero e in fondo un gradiente numerico diciamo}
	\end{frame}
	
	\begin{frame}{Intermezzo: cenni di \emph{Machine Learning}}
		\begin{block}{Remark 1:} 
			ML $\cong$ minimizzare la \emph{loss function} $\mathcal{L}(\Theta;X_0)$. {\smaller ($\Theta$ racchiude tutti i parametri del modello)}
		
			Esempio: $\mathcal{L}(\Theta;X_0)=\frac{1}{k}\sum_{l=1}^k\norm{X_N[:\mid\!l] - y^{(l)}}_2^2$.
			\medskip
		
			Tipicamente, $\mathcal{L}(\Theta;X_0)\equiv L(X_N)$, con $L\colon\R{n_N}{k}\to\R$ e $X_N=X_N(\Theta;X_0)$. 
			\medskip
		
			Si ha $\dd{\mathcal{L}}=\qty(\grad{\mathcal{L}})^{\!\!\top\bullet}\dd{\Theta}$. Allo stesso tempo, per la regola della catena: $\dd{\mathcal{L}}=\pdv{L}{X_N}\dd{X_N}=\qty(\grad{L})^{\!\top\bullet}\dd{X_N}$, e a sua volta 
			$\dd{X_N}=\pdv{X_N}{\Theta}\dd{\Theta}$.
		
			Dunque $\dd{L}=\left\langle\qty(\pdv{X_N}{\Theta})^{\!\!\top}\grad{L},\dd{\Theta}\right\rangle_F$.
			\smallskip
			
			La vera sfida: applicare $\qty(\pdv{X_N}{\Theta})^{\mathclap{\top}}$ a $\grad{L}$, per ottenere $\grad_{\Theta}\mathcal{L}$. Ciò corrisponde al calcolo dei \emph{path weights}.
		\end{block}
		\begin{block}{Remark 2:}
			Il \emph{bias} viene aggiunto indipendentemente a ciascun \emph{pattern} che viene processato. Nel layer $i$ si ha: $W_iX_{i-1}\,.\!+b_i\equiv W_iX_{i-1}+B_i$, dove $B_i\coloneqq b_ie^{\top}$.
		\end{block}
	\end{frame}
	
	\begin{frame}{Pesi degli archi $\qquad 1/2$}{Differenziale della funzione di attivazione}
		Ci proponiamo di scrivere la matrice di adiacenza del grafo computazionale del MLP nel caso generale.
		
		\begin{definizione}
			Per $h\colon\R\to\R$, l'applicazione \emph{pointwise} di $h$ è
			\vspace*{-0.5em}
			\[
				\begin{array}{rccl}
					h.\colon & \R{m}{n} & \longrightarrow & \R{m}{n} \\
							 & A	 	& \longmapsto	  & \qty(h(a_{i,j}))_{\substack{i\in[m]\\ j\in[n]}}
				\end{array}
			\]
		\end{definizione}
		\onslide<2->{Ricordiamo che $\dd h.(A)=\qty(h.(A))'[\dd{A}]$ {\smaller (differenziale di Fréchet)}.}
		\onslide<3->{\begin{align*}
			h.(A + \dd A) - h.(A) & = \qty(h(a_{i,j}+\dd{a}_{i,j}) - h(a_{i,j}))_{i,j} 
			= \qty(h'(a_{i,j})\dd{a_{i,j}})_{i,j}\\
			 & = h'.(A)\,.\!* \dd{A} \equiv \qty(h'.(A))_{\mathrm{H}}[\dd{A}].
		\end{align*}}
		\onslide<4->{Dunque $\dd h.(A)$ è la funzione $\dd{A}\mapsto\qty(h'.(A))_{\mathrm{H}}[\dd{A}]$.}
		\medskip 
		
		\onslide<5->{Il particolare per il MLP, $\dd{h.}(W_iX_{i-1}+b_ie^{\top})=(\Delta_i)_{\mathrm{H}}\,\dd\qty(W_iX_i+b_ie^{\top})$}
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:layer_differential}{Pesi degli archi $\qquad 2/2$}}
		{Differenziale del layer $i$-esimo}
		Dunque il differenziale del layer $i$-esimo è dato da
		\begin{align*}
			\dd{X_i} & = \dd\qty(h_i.\qty(W_iX_{i-1} + b_ie^{\top})) =
			\qty(\Delta_i)_{\mathrm{H}}\qty[\dd\qty(W_iX_{i-1}+B_i)]
		\onslide<2->{\intertext{$W_i$ e $B_i$ sono parametri, ma la dipendenza da parametri si nasconde anche dentro $X_{i-1}$, dunque (cfr. Appendice~\hyperlink{frame:layer_differential:appendice}{\faHandPointRight})}}
			\onslide<3->{
			\dd{X_i} & = {\Delta_i}_{\mathrm{H}}\qty[\dd{W_i}X_{i-1}+W_i\dd{X_{i-1}}+\dd{B_i}]\\
			}
			\onslide<4->{
				& = \underbrace{\qty({\Delta_i}_{\mathrm{H}}\circ{X_{i-1}}_{\mathrm{R}})}_{\pdv{X_i}{W_i}}\,\dd{W_i}
				  +	\underbrace{\qty({\Delta_i}_{\mathrm{H}}\circ{W_{i}}_{\mathrm{L}})}_{\pdv{X_i}{X_{i-1}}}\,\dd{X_{i-1}}
				  + \underbrace{\qty({\Delta_i}_{\mathrm{H}}\circ{e^{\top}}_{\mathrm{R}})}_{\pdv{X_i}{b_i}}\,\dd{b_i}
			}
		\end{align*}		
		
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:loss_derivative}{Derivata della \emph{loss}}}
		{Derivata della loss rispetto ad alcuni suoi parametri}
		%Possiamo scrivere la derivata della \emph{loss} rispetto ai parametri
		Seguendo i \emph{path weights} direttamente sul grafo computazionale, procedendo dalla sorgente al pozzo, si ha
		\begin{align*}
			\pdv{\mathcal{L}}{W_i} & =
				\underbrace{\grad{L}^{\top\bullet}}_{\pdv{\mathcal{L}}{X_N}}\ \underbrace{(\Delta_N)_{\mathrm{H}}\,(W_N)_{\mathrm{L}}}_{\pdv{X_N}{X_{N-1}}}\,\ldots\,    \underbrace{(\Delta_{i+1})_{\mathrm{H}}\,(W_{i+1})_{\mathrm{L}}}_{\pdv{X_{i+1}}{X_i}}\ 
				\underbrace{(\Delta_i)_{\mathrm{H}}\,(X_{i-1})_{\mathrm{R}}}_{\pdv{X_{i}}{W_i}}\\
			\pdv{\mathcal{L}}{b_i} & =
				\underbrace{\grad{L}^{\top\bullet}}_{\pdv{\mathcal{L}}{X_N}}\ \underbrace{(\Delta_N)_{\mathrm{H}}\,(W_N)_{\mathrm{L}}}_{\pdv{X_N}{X_{N-1}}}\,\ldots\,    \underbrace{(\Delta_{i+1})_{\mathrm{H}}\,(W_{i+1})_{\mathrm{L}}}_{\pdv{X_{i+1}}{X_i}}\ 
				\underbrace{(\Delta_i)_{\mathrm{H}}\,(e^{\!\top})_{\mathrm{R}}}_{\pdv{X_{i}}{b_i}}\\  
		%\end{align*}
		\intertext{Per le proprietà del $\colorbox{gray!30}{\phantom{$\cdot$}}^{\top\bullet}\!$ si ha (cfr. Appendice~\hyperlink{frame:loss_derivative:appendice}{\faHandPointRight})}
		%\begin{align*}
			\pdv{\mathcal{L}}{W_i} & = \Big(
				\big(X_{i-1}^{\top}\big)_{\mathrm{R}}\qty(\Delta_i)_{\mathrm{H}}\ 
				\big(W_{i+1}^{\top}\big)_{\mathrm{L}}\qty(\Delta_{i+1})_{\mathrm{H}}\,\ldots\,
				\big(W_N^{\top}\big)_{\mathrm{L}}\qty(\Delta_N)_{\mathrm{H}}\ 
				\grad{L}\Big)^{\!\top\bullet}\\
			\pdv{\mathcal{L}}{b_i} & = \Big(
				e_{\mathrm{R}}\qty(\Delta_i)_{\mathrm{H}}\ 
				\big(W_{i+1}^{\top}\big)_{\mathrm{L}}\qty(\Delta_{i+1})_{\mathrm{H}}\,\ldots\,
				\big(W_N^{\top}\big)_{\mathrm{L}}\qty(\Delta_N)_{\mathrm{H}}\ 
				\grad{L}\Big)^{\!\top\bullet}\\
		\end{align*}
		\vspace*{-3em}
		
		che corrisponde ad accumulare i pesi visitando il grafo al contrario.
	\end{frame}
		
	\begin{frame}{Derivata della \emph{loss}}{Ricavare il gradiente dalla matrice di adiacenza}
		La matrice di adiacenza del MLP ha come elementi i pesi degli archi, dunque operatori lineari. \onslide<2->{Per quanto visto, il gradiente della \emph{loss} è dato da}
		\onslide<2->{\begin{multline*}
			\grad{\mathcal{L}} = 
			\begin{pNiceMatrix}[nullify-dots]
				{\Delta_1}_{\mathrm{H}}\circ{X_0}_{\mathrm{R}} & {\Delta_1}_{\mathrm{H}}\circ{e^{\!\top}}_{\mathrm{R}} & & & & \\
				& \Ddots & \Ddots & & & \\
				&		 & 		  & & & \\
				&		 & 		  & & & \\
				& & & & {\Delta_3}_{\mathrm{H}}\circ{X_2}_{\mathrm{R}} & {\Delta_3}_{\mathrm{H}}\circ{e^{\!\top}}_{\mathrm{R}}
			\end{pNiceMatrix}^{\mathclap{\top}}\cdot\\
			\begin{pNiceMatrix}[nullify-dots]
				\mathcal{I} & & & & & \\
				-{\Delta_2}_{\mathrm{H}}\circ{W_2}_{\mathrm{L}} & \mathcal{I} & & & & \\
				&             &             &             &             & \\
				&             &             &             &             & \\
				&             &             &             & \mathcal{I} & \\
				&             &             &             & -{\Delta_N}_{\mathrm{H}}\circ{W_N}_{\mathrm{L}} & \mathcal{I} \\
				\CodeAfter
				\tikz[loosely dotted, line width=0.8pt]
				\draw (2-2.east) -- (5-5.north west)
				(2-1.south) -- (6-5.north west);
			\end{pNiceMatrix}^{\mathclap{-\!\top}}\cdot 
			\begin{pNiceMatrix}[nullify-dots]
				\mathcal{O}\\
				\Vdots
				\\
				\\
				\\
				\mathcal{O}\\
				\grad_{X_n}{L}
			\end{pNiceMatrix}	
		\end{multline*}}
	
		\blankfootnote{\onslide<2->{$\mathcal{I}$: operatore identità, $\mathcal{O}$: operatore nullo}}
	\end{frame}
	
	
	
	
	
	
	
	
	
	
    
    \begin{frame}
        \begin{center}
            \Huge{Grazie per l'attenzione!}
        \end{center}
    \end{frame}
    
    \begin{frame}{\refname}
    	\begin{thebibliography}{9}
    		\bibitem{article:BackBackBack} A. Edelman, E. Akyurek, Y. Wang
    		\newblock Backpropagation through Back Substitution with a Backslash
    		\newblock SIAM J. Matrix Anal. Appl. Vol~45 (2024), No.~1, pp.~429-449
    	\end{thebibliography}
    \end{frame}
    
\appendix
\section*{Appendice}
	\begin{frame}
		\begin{center}
			\Huge{\textbf{Appendice}}
		\end{center}
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:path_product_no_top_sort:appendice}{Matrice dei \emph{path products}, senza ordinamento topologico}}
		Sia $L^{\top}\in\R{n}{n}$ la matrice di adiacenza di un DAG $\mathcal{G}$. L'operazione corrispondente alla rietichettatura dei nodi di $\mathcal{G}$ è il coniugio con una matrice di permutazione, dunque $\exists\,P\in\R{n}{n}$ di permutazione tale che $\hat{L}^{\top}\coloneqq P L^{\top} P^{\top}$ è strettamente triangolare superiore. 
		
		\begin{block}{}
			Nella nuova etichettatura, i primi $s$ nodi sono sorgenti e gli ultimi $p$ sono pozzi. Nella vecchia etichettatura, i nodi sorgente sono indicizzati da $P^{\top}e_i$ per $i\in[s]$ {\smaller ($e_i$ è l'$i$-esimo versore canonico)}
		\end{block}
		
		
		Si ha $(I-\hat{L}^{\top})=P(I-L^{\top})P^{\top}$, da cui $(I-\hat{L})^{-\!\top}=P(I-L)^{-\!\top}P^{\top}$.
		Dunque
		\[\texttt{path\_weights} =
		\begin{pNiceMatrix}%{cc}
			\Block{2-3}<\large>{I_s} & & \\
			& & \\
			\Hline
			\Block{2-3}<\large>{0} & & \\
			& & \\
		\end{pNiceMatrix}^{\mathllap{\top}}
		\qty(I - \hat{L})^{-\!\top}
		\begin{pNiceMatrix}%{cc}
			\Block{3-2}<\large>{0} & \\
			& \\
			& \\
			\Hline
			\Block{1-2}{I_p} 	   & \\
		\end{pNiceMatrix} =
				\begin{pNiceMatrix}%{cc}
			\Block{2-3}<\large>{I_s} & & \\
			& & \\
			\Hline
			\Block{2-3}<\large>{0} & & \\
			& & \\
		\end{pNiceMatrix}^{\mathllap{\top}}
		P\qty(I - L)^{-\!\top}P^{\top}
		\begin{pNiceMatrix}%{cc}
			\Block{3-2}<\large>{0} & \\
			& \\
			& \\
			\Hline
			\Block{1-2}{I_p} 	   & \\
		\end{pNiceMatrix}
		\]
		
		Ci siamo ricondotti al caso precedente. I path weights sono dati dagli elementi di $(I-L)^{-\!\top}$ di indici dati dai nodi sorgente/pozzo secondo l'enumerazione originale.
		
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:path_product_no_top_sort}{\faHandPointLeft}}
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:backsub_is_backprop:appendice}{Backsubstitution $\equiv$ backpropagation $\qquad 1/2$}}
		{Formule della sostituzione all'indietro}
		
		Ricordiamo che la risoluzione $Tx=b$ tramite sostituzione all'indietro è data da
		\[
		\begin{dcases}
			x_n = b_n / T_{n,n} & \\
			x_k = \frac{1}{T_{k,k}}\smashoperator[r]{\sum_{j=k+1}^n} T_{k,j}x_k & k=n-1,\dots,1
		\end{dcases}
		\]
		
		Supponiamo $\mathcal{G}$ topologicamente ordinato. Allora la matrice di adiacenza $L^{\top}\in\R{n}{n}$ è strettamente triangolare superiore. Consideriamo per semplicità un singolo pozzo, rappresentato da $e_n$. La risoluzione di $(I-L)^{\top}x=e_n$ per sostituzione all'indietro diventa
		\begin{equation}\label{eqn:backsub_ImLt}
		\begin{dcases}
			x_n = 1/1 = 1& \\
			x_k = \smashoperator[lr]{\sum_{j=k+1}^n} (I-L^{\top})_{k,j}\,x_k = \smashoperator[lr]{\sum_{j=k+1}^n} (L^{\top})_{k,j}\,x_j
		\end{dcases}	
		\end{equation}
	
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:backsub_is_backprop}{\faHandPointLeft}}
	\end{frame}
				
	\begin{frame}
		{Backsubstitution $\equiv$ backpropagation $\qquad 2/2$}
		{Dimostrazione}
		
		\vspace{-1em}
		\begin{proposizione}
			$x_k$ restituito dalla~\eqref{eqn:backsub_ImLt} accumula {\smaller ($\equiv$ somma)} tutti i path product da $k$ a $n$.
		\end{proposizione}	
		\begin{block}{Dimostrazione.}
			Per induzione su $k=n,\dots,1$.
			\begin{itemize}
				\item \textbf{Passo base}: $x_n=1$ è banalmente vero
				\item \textbf{Passo induttivo}: consideriamo $\hat{\jmath}>k$. Poiché $(L^{\top})_{k,\hat{\jmath}}$ è il peso dell'arco $k\to\hat{\jmath}$ e $x_{\hat{\jmath}}$ è --per ipotesi induttiva -- il path product del cammino $\hat{\jmath}\to n$, 
				$(L^{\top})_{k,\hat{\jmath}}\,x_{\hat{\jmath}}$ è il path product del cammino da $k$ a $n$ che passa da $\hat{\jmath}$ come primo passo.
				\smallskip 
				
				Dunque ciascun addendo di~\eqref{eqn:backsub_ImLt} è il path product di un cammino da $k$ a $n$.
				\smallskip 
				
				Ne segue che $x_k = \sum_{j=1}^n (L^{\top})_{k,j}\,x_j = \sum_{j=k+1}^n (L^{\top})_{k,j}\,x_j$, poiché $L^{\top}$ è triangolare superiore. \qedhere
			\end{itemize}
		\end{block}
		
		Di conseguenza, risolvere $(I-L)^{\top}x=e_n$ tramite sostituzione all'indietro corrisponde a calcolare i path weights visitando $\mathcal{G}$ dal pozzo alle sorgenti.
		
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:backsub_is_backprop}{\faHandPointLeft}}
	\end{frame}
	
	
	\begin{frame}
		{\hypertarget{frame:different_orders:appendice}{Strategie alternative per valutare \texttt{path\_weights}}}
		{Ciascuna strategia di valutazione corrisponde a un diverso ordine di visita}
		
		$I-L^{\top}$ è triangolare superiore, e ha $1$ sulla diagonale. \`E possibile scrivere $(I-L)^{\top}$ come prodotto di matrici elementari di Gauss. Ciò corrisponde a fare eliminazione gaussiana (per colonne) per ridurre $I-L^{\top}$ all'identità, e poi invertire: $I-L^{\top}=E_1\cdot\ldots\cdot E_m$ dove ciascuna $E_i$ è una matrice elementare di Gauss.
		Dunque 
		\[
		\texttt{path\_weights}=
		\left\lgroup\begin{matrix}
			I_s \\ 
			\hline
			0
		\end{matrix}\right\rgroup^{\mathclap{\top}}
		(I-L)^{-\!\top}
		\left\lgroup\begin{matrix}
			0 \\ 
			\hline
			I_p
		\end{matrix}\right\rgroup = 
		\left\lgroup\begin{matrix}
			I_s \\ 
			\hline
			0
		\end{matrix}\right\rgroup^{\mathclap{\top}}
		E_m^{-1}\cdot\ldots\cdot E_1^{-1}
		\left\lgroup\begin{matrix}
			0 \\ 
			\hline
			I_p
		\end{matrix}\right\rgroup.		
		\]
		
		La scelta di un ``inserimento di parentesi'' restituisce un ordine nel calcolo dei path products. Moltiplicare due matrici elementari corrisponde a lavorare su sottografi {\smaller (es. calcolare path weights in \emph{reverse mode} su un sottografo)}; ossia a calcolare path products ``parziali'' che poi vengono utilizzati per calcolare path products di cammini che hanno quello già considerato come sottocammino.
		
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:backsub_is_backprop}{\faHandPointLeft}}
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:adding_loss:appendice}{Aggiunta di un nodo al grafo computazionale $\qquad 1/2$}}
	
		Supponiamo che $\mathcal{G}$ abbia $s$ sorgenti {\smaller (etichettati $1,\dots,s$)} e solo un nodo pozzo {\smaller (etichettato $n$)}. Si ha
		$
			\texttt{path\_weights}=
			\left(\begin{array}{c|c}
				I_s & 0\\ 
			\end{array}\right)
			(I-L)^{-\!\top}e_n.
		$
		\smallskip
		
		Vogliamo aggiungere un ulteriore nodo, e un arco $n\to n+1$ con peso $w$. L'operazione corrispondente è
		\[
			\texttt{new\_path\_weights}=
			\begin{pNiceMatrix}%{cc}
				\Block{2-3}<\large>{I_s} & & \\
				& & \\
				\Hline
				\Block{2-3}<\large>{0} & & \\
				& & \\
				\Hline[tikz=dashed]
				0 & \Cdots & 0
			\end{pNiceMatrix}^{\mathllap{\top}}
			\underbrace{\begin{pNiceArray}{ccccc|[tikz=dashed]c}
				\Block{4-5}{(I-L)^{-\!\top}} & & & & & \\
				 & & & & & \\
				 & & & & & \\
				 & & & & & \\
				\Hline[tikz=dashed]
				 & & & & & 1
			\end{pNiceArray}
			\begin{pNiceArray}{cccc|[tikz=dashed]c}[nullify-dots]
				\mathsmaller{1} & 	     & &   				 & \mathsmaller{0}\\
				  				& \Ddots & &   				 & \Vdots \\
				  				&		 & &   				 & \mathsmaller{0}\\
				  				& 	     & & \mathsmaller{1} & w\\
				\Hline[tikz=dashed]
				  & & & & 1
			\end{pNiceArray}}_{\text{nuova matrice dei \emph{path products}}}
			\begin{pNiceMatrix}[nullify-dots]
				\mathsmaller{0} \\
				\Vdots \\
				\\
				\mathsmaller{0} \\
				\Hline[tikz=dashed]
				1
			\end{pNiceMatrix}
		\]
		
		\textbf{Idea}: i \emph{path weights} del nodo $n+1$ sono ottenuti da quelli del nodo $n$, moltiplicandoli per $w$. {\smaller (La moltiplicazione è a destra, perché $w$ è l'ultimo arco percorso!)}
		\smallskip
		
		Quindi la nuova matrice dei \emph{path products} ha una colonna in più, in cui tutti gli elementi sono uguali a quelli della colonna precedente, moltiplicati per $w$. 
		\smallskip
		
		In termini di algebra lineare, ciò corrisponde all'operazione rappresentata.
		
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:adding_loss}{\faHandPointLeft}}
	\end{frame}
	
	\begin{frame}{Aggiunta di un nodo al grafo computazionale $\qquad 2/2$}
		\[
			\texttt{new\_path\_weights}=
			\begin{pNiceMatrix}%{cc}
				\Block{2-3}<\large>{I_s} & & \\
				& & \\
				\Hline
				\Block{2-3}<\large>{0} & & \\
				& & \\
				\Hline[tikz=dashed]
				0 & \Cdots & 0
			\end{pNiceMatrix}^{\mathllap{\top}}
			\underbrace{\begin{pNiceArray}{ccccc|[tikz=dashed]c}
					\Block{4-5}{(I-L)^{-\!\top}} & & & & & \\
					& & & & & \\
					& & & & & \\
					& & & & & \\
					\Hline[tikz=dashed]
					& & & & & 1
				\end{pNiceArray}
				\begin{pNiceArray}{cccc|[tikz=dashed]c}[nullify-dots]
					\mathsmaller{1} & 	     & &   				 & \mathsmaller{0}\\
					& \Ddots & &   				 & \Vdots \\
					&		 & &   				 & \mathsmaller{0}\\
					& 	     & & \mathsmaller{1} & w\\
					\Hline[tikz=dashed]
					& & & & 1
			\end{pNiceArray}}_{\text{nuova matrice dei \emph{path products}}}
			\begin{pNiceMatrix}[nullify-dots]
				\mathsmaller{0} \\
				\Vdots \\
				\\
				\mathsmaller{0} \\
				\Hline[tikz=dashed]
				1
			\end{pNiceMatrix}.
		\]
		\noindent Svolgendo le operazioni, si ottiene
		\vspace*{-1em}
		\[
			\texttt{new\_path\_weights}=
			\left(\begin{array}{c|c}
				I_s & 0\\
			\end{array}\right)
			\qty(I - L)^{-\!\top}
			\left(\begin{matrix}%[nullify-dots]
				\mathsmaller{0} \\
				\vdots \\
				\mathsmaller{0} \\
				\mathsmaller{w}
			\end{matrix}\right).
			%\left({\footnotesize {0},\dots,{0},{w}}\right)^{\top}
		\]
		Il sistema lineare ha la stessa dimensione del precedente, cambia solo il termine noto.
		Non dipendendo dalla ``forma'' di $w$, il ragionamento è valido anche più in generale, con operatori come elementi. %{\smaller (a patto di fare le dovute sostituzioni, ad esempio sostituire i $0$ e $1$ con gli operatori nulli e identità, rispettivamente)}
		
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:adding_loss}{\faHandPointLeft}}
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:layer_differential:appendice}{Dettagli sul calcolo del differenziale del layer $i$-esimo $\qquad 1/2$}}
		
		Per la regola della catena, $\dd{X_i}=\pdv{X_i}{(W_iX_{i-1}+B_i)}\dd{(W_iX_{i-1}+B_i)}$, da cui 
		$\dd{X_i}=\qty(\Delta_i)_{\mathrm{H}}\qty[\dd\qty(W_iX_{i-1}+B_i)]$. 
		\medskip 
		
		Per esplicitare il differenziale di $W_iX_{i-1}+b_ie^{\top}$ rispetto ai parametri, si considerano tre casi: 
		\begin{enumerate}
			\item $X_{i-1}\mapsto W_iX_{i-1}+B_i\equiv X_i\mapsto {W_i}_{\mathrm{L}}\qty[X_{i-1}]+B_i$
			\item $W_i\mapsto W_iX_{i-1}+B_i\equiv W_i\mapsto {X_{i-1}}_{\mathrm{R}}\qty[W_i]+B_i$
			\item $b_i\mapsto W_iX_{i-1}+b_ie^{\top}\equiv b_i\mapsto W_iX_{i-1}+{e^{\top}}_{\mathrm{R}}[b_i]$
		\end{enumerate}
		\smallskip 
		
		In tutti e tre abbiamo una funzione affine. Quindi {\smaller (in qualsiasi punto)} il differenziale è dato dalla parte lineare: 
		\begin{enumerate}
			\item $\pdv{X_{i-1}}\qty(W_iX_{i-1}+B_i)={W_i}_{\mathrm{L}}$, ossia la funzione $\dd{X}\mapsto{W_i}_{\mathrm{L}}[\dd{X}]$
			\item $\pdv{W_i}\qty(W_iX_{i-1}+B_i)={X_{i-1}}_{\mathrm{R}}$, ossia la funzione $\dd{W}\mapsto{X_{i-1}}_{\mathrm{R}}[\dd{W}]$
			\item $\pdv{b_i}\qty(W_iX_{i-1}+B_i)={e^{\top}}_{\mathrm{R}}$, ossia la funzione $\dd{b}\mapsto{e^{\top}}_{\mathrm{R}}[\dd{b}]$
		\end{enumerate}
		
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:layer_differential}{\faHandPointLeft}}	
	\end{frame}
	
	\begin{frame}{Dettagli sul calcolo del differenziale del layer $i$-esimo $\qquad 2/2$}
		
		Mettendo tutto assieme,
		\begin{align*}
			\dd{X_i} & = \qty(\Delta_i)_{\mathrm{H}}\qty[\dd\qty(W_iX_{i-1}+B_i)]=
			{\Delta_i}_{\mathrm{H}}\qty[{X_{i-1}}_{\mathrm{R}}\,W_i + {W_i}_{\mathrm{L}}\,X_{i-1}+{e^{\top}}_{\mathrm{R}}\,b_i]\\
			 & = \underbrace{\qty({\Delta_i}_{\mathrm{H}}\circ{X_{i-1}}_{\mathrm{R}})}_{\pdv{X_i}{W_i}}\,\dd{W_i}
			   + \underbrace{\qty({\Delta_i}_{\mathrm{H}}\circ{W_{i}}_{\mathrm{L}})}_{\pdv{X_i}{X_{i-1}}}\,\dd{X_{i-1}}
			   + \underbrace{\qty({\Delta_i}_{\mathrm{H}}\circ{e^{\top}}_{\mathrm{R}})}_{\pdv{X_i}{b_i}}\,\dd{b_i}.
		\end{align*}
		
		Questo mostra che 
		\[
			\dd{X_i}=\pdv{X_i}{W_i}\dd{W_i} + \pdv{X_i}{X_{i-1}}\dd{X_{i-1}} + \pdv{X_i}{b_i}\dd{b_i},
		\]
		infatti per la regola della catena $\pdv{X_i}{\star}=\pdv{X_i}{(X_{i-1}W_i+B_i)}\textcolor{red}{\cdot}\pdv{(X_{i-1}W_i+B_i)}{\star}$; abbiamo visto che $\pdv{X_i}{(X_{i-1}W_i+B_i)}={\Delta_i}_{\mathrm{H}}$ e ad esempio  {\smaller ($\star = W_i$)} $\pdv{(X_{i-1}W_i+B_i)}{W_i}={X_{i-1}}_{\mathrm{R}}$
		
		\begin{oss}
			Il $\textcolor{red}{\cdot}$ è il prodotto ``giusto'' a seconda del contesto: composizione {\smaller (se le derivate parziali sono pensate come operatori lineari)}, o prodotto matriciale {\smaller (se invece le derivate parziali sono viste ``in coordinate'', pensate come ``gli oggetti che agiscono'')}
		\end{oss}
		
	
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:layer_differential}{\faHandPointLeft}}
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:loss_derivative:appendice}{Aggiunti di operatori lineari $\qquad 1/4$}}
		{}
		Abbiamo visto 
		\[
			\pdv{L}{W_i}=\pdv{L}{X_N}\pdv{X_N}{W_i}=\grad{L}^{\!\top\bullet}\overbrace{\pdv{X_N}{W_i}}^{\coloneqq M_i}.
		\]
		Per il lemma,
		\[
			\grad{L}^{\!\top\bullet}M_i=\qty(M_i^{\top}\grad{L})^{\!\top\bullet}.
		\]
		Per la regola della catena, $M_i$ è dato dalla composizione di operatori lineari:
		\[
			M_i=\underbrace{M_i^{(1,N)}M_i^{(2,N)}}_{\pdv{X_N}{X_{N-1}}}\ldots
				\underbrace{M_i^{(1,i+1)}M_i^{(2,i+1)}}_{\pdv{X_{i+1}}{X_i}}
				\underbrace{M_i^{(1,i)}M_i^{(2,i)}}_{\pdv{X_i}{W_i}}.
		\]
		$M^{\top}$ si ottiene componendo gli aggiunti dei singoli fattori in ordine inverso:
		\[
			M^{\top}=\qty(M_i^{(1,N)}M_i^{(2,N)}\ldots M_i^{(1,i)}M_i^{(2,i)})^{\top}=
					{M_i^{(2,i)}}^{\top}{M_i^{(1,i)}}^{\top}\!\!{\dots}\,{M_i^{(2,N)}}^{\top}{M_i^{(1,N)}}^{\top}
		\]
		
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:loss_derivative}{\faHandPointLeft}}		
	\end{frame}
	
	\begin{frame}
		{\hypertarget{frame:adjoints:appendice}{Aggiunti di operatori lineari $\qquad 2/4$}}
		{L'aggiunta di $A_{\mathrm{R}}$}
		
		Convenendo che $\qty(\phantom{A})_{\mathrm{R}}$ agisca su matrici con $m$ righe, per $A\in\R{n_1}{n}$ la funzione		
		$(A^{\top})_{\mathrm{R}}\colon\R{m}{n}\to\R{m}{n_1}$ ha ``tipo'' compatibile con l'aggiunta di $A_{\mathrm{R}}$.
		
		\begin{lemma}
			Sia $A\in\R{n_1}{n}$. Si ha $(A_{\mathrm{R}})^{\!\top}=(A^{\top})_{\mathrm{R}}$
		\end{lemma}
		\begin{proof}
			Per ogni $U\in\R{m}{n}$, $\forall\,V\in\R{m}{n_1}$ si ha
			\begin{align*}
				\langle(A^{\top})_{\mathrm{R}}U, V\rangle_{F} & =\tr(\qty(UA^{\top})^{\top}V)=\tr(AU^{\top}V)\\
			\intertext{Per la proprietà ciclica della traccia,}
				& = \tr(U^{\top}VA)=\langle U,VA\rangle_F=\langle U, A_{\mathrm{R}}\,V\rangle_{F}.
			\end{align*}
			Dunque $(A^{\top})_{\mathrm{R}}$ è l'aggiunta di $A_{\mathrm{R}}$.
		\end{proof}
		
		\blankfootnote{
			\textbf{Indietro:}  (definizioni~\hyperlink{frame:adjoints}{\faHandPointLeft})$\quad$ 
			(derivata della \emph{loss}~\hyperlink{frame:loss_derivative}{\faHandPointLeft})
		}
	\end{frame}
	
	\begin{frame}{Aggiunti di operatori lineari $\qquad 3/4$}{L'aggiunta di $B_{\mathrm{L}}$}
		Convenendo che $\qty(\phantom{B})_{\mathrm{R}}$ agisca su matrici con $n$ colonne, per $B\in\R{m_1}{m}$ la funzione $(B^{\top})\colon\R{m_1}{n}\to\R{m}{n}$ ha ``tipo'' compatibile con l'aggiunta di $B_{\mathrm{R}}$.
		
		\begin{lemma}
			Sia $B\in\R{m_1}{m}$. Si ha $(B_{\mathrm{L}})^{\top}=(B^{\top})_{\mathrm{L}}$
		\end{lemma}
		\begin{proof}
			Per ogni $U\in\R{m_1}{m}$, $\forall\,V\in\R{m}{n}$ si ha
			\[
				\left\langle(B^{\top})_{\mathrm{L}}U,V\right\rangle_F=\langle B^{\top}U,V\rangle_F=\tr((B^{\top}U)^{\top}V)=\tr(U^{\top}BV)=\left\langle U, (B)_{\mathrm{L}}V\right\rangle_F.
			\]
			Dunque $(B^{\top})_{\mathrm{L}}$ è l'aggiunta di $B_{\mathrm{L}}$.
		\end{proof}
		
		\textbf{Idea}: passando alla vettorizzazione, $B_{\mathrm{L}}$ è rappresentato da $I\otimes B$, e $(I\otimes B)^{\top}=I\otimes B^{\top}$. $I\otimes B^{\top}$ rappresenta $(B^{\top})_{\mathrm{L}}$.
		
		\blankfootnote{
			\textbf{Indietro:}  (definizioni~\hyperlink{frame:adjoints}{\faHandPointLeft})$\quad$ 
			(derivata della \emph{loss}~\hyperlink{frame:loss_derivative}{\faHandPointLeft})
		}
	\end{frame}
	
	\begin{frame}{Aggiunti di operatori lineari $\qquad 4/4$}{L'aggiunta di $M_{\mathrm{H}}$}
		Ricordiamo che:
		\begin{itemize}
			\item $\phi\colon\qty(V,\langle\cdot,\cdot\rangle_V)\to\qty(W,\langle\cdot,\cdot\rangle_W)$ è un'\textbf{isometria} se $\langle v_1,v_2\rangle_V=\langle\phi(v_1),\phi(v_2)\rangle_W$ per ogni $v_1,v_2\in V$
			\item L'inverso di un isomorfismo isometrico coincide con il suo aggiunto: $\phi^{-1}=\phi^{\top}$
		\end{itemize}
		
		Si vede a occhio che $\operatorname{Vec}\colon\qty(\R{m}{n},\langle\cdot,\cdot\rangle_F)\to\qty(\R{mn},\langle\cdot,\cdot\rangle_2)$ è un'isometria %{\smaller (è fatta apposta\dots)}
		
		\begin{lemma}
			Sia $M\in\R{m}{n}$. La funzione $M_{\mathrm{H}}$ è autoaggiunta: $(M_{\mathrm{H}})^{\top}=M_{\mathrm{H}}$
		\end{lemma}
		\begin{proof}
			Sia $\R{mn}{mn}\ni\tilde{M}\coloneqq\operatorname{Diag}(\operatorname{Vec}(M))$, e sia 
			$f_{\tilde{M}}\colon\R{mn}\to\R{mn}$ t.c. $f_{\tilde{M}}(v)=\tilde{M}v$. Essendo $\tilde{M}$ diagonale, $f_{\tilde{M}}$ è chiaramente autoaggiunta. 
			\smallskip
			
			Si ha $M_{\mathrm{H}}=\operatorname{Vec}^{-1}\circ f\circ\operatorname{Vec}$. Essendo $\operatorname{Vec}$ un'isometria, 
			\[
				(M_{\mathrm{H}})^{\top}=\operatorname{Vec}^{\top}\circ f^{\top}\circ\operatorname{Vec}^{-\top}=\operatorname{Vec}\circ f\circ\operatorname{Vec}^{-1}=M_{\mathrm{H}}.\qedhere
			\]
		\end{proof}
			
		\blankfootnote{
			\textbf{Indietro:}  (definizioni~\hyperlink{frame:adjoints}{\faHandPointLeft})$\quad$ 
								(derivata della \emph{loss}~\hyperlink{frame:loss_derivative}{\faHandPointLeft})
		}
	\end{frame}

\end{document}

