{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57389848",
   "metadata": {},
   "source": [
    "# Problema 3: la forma di `M`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be4f6a",
   "metadata": {},
   "source": [
    "In breve, nel Notebook originale `M` √® diagonale ma ha degli array come elementi, con l'idea che i nodi sorgente racchiudano sia i pesi sia il bias del layer (aka nodo interno) corrispondente.\n",
    "\n",
    "Se per√≤ volessimo distinguere questi input?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e534b68",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b16826",
   "metadata": {},
   "source": [
    "Facciamo veloce, che tanto oramai li conosciamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a6d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, Revise\n",
    "\n",
    "Revise.includet(joinpath(@__DIR__,\"..\",\"Operator.jl\"))\n",
    "\n",
    "using .MyOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf05b4a",
   "metadata": {},
   "source": [
    "***OSS***: nel modulo `.Operator` ci sono pi√π metodi di quelli presenti nell'articolo originale!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935525c",
   "metadata": {},
   "source": [
    "### La rete neurale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6e19a",
   "metadata": {},
   "source": [
    "Riusiamo le celle di codice gi√† viste..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08e3156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neural_net (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using OffsetArrays\n",
    "\n",
    "# funzione di attivazione\n",
    "h(x)  = tanh(x)\n",
    "h‚Ä≤(x) = 1 - h(x)^2\n",
    "\n",
    "\n",
    "# questa funzione, di fatto, effettua il forward pass\n",
    "function neural_net(params,X‚ÇÄ;h=h,h‚Ä≤= h‚Ä≤)\n",
    "    T = Matrix{Float64}\n",
    "    N = length(params)\n",
    "    X = OffsetArray(Vector{T}(undef,N+1),0:N)   \n",
    "    Œî = Vector{T}(undef, N)\n",
    "    X[0] = X‚ÇÄ\n",
    "    W = first.(params)\n",
    "    B = last.(params)\n",
    "    \n",
    "    for i=1:N         \n",
    "          X[i] =  h.(W[i]*X[i-1] .+ B[i])\n",
    "          Œî[i] =  h‚Ä≤.(W[i]*X[i-1] .+ B[i])        \n",
    "    end \n",
    "    X,Œî\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9493b83",
   "metadata": {},
   "source": [
    "Adesso possiamo costruire la rete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89652eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Un po' di parametri che descrivono la rete...\n",
    "\n",
    "n = [5,4,3,1]   # contiene [n‚ÇÄ...n_N], le dimensioni dei layer\n",
    "k = 10          # batchsize\n",
    "N = length(n)-1 # numero di layer (nascosti + output). Dev'essere positivo\n",
    "\n",
    "init(sizes...) = 0.01randn(sizes...)    # utility function per inizializzare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a46a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[-0.0006933712885586175 -0.022213780060294074 ‚Ä¶ 0.0022010053899360767 -0.008681635089007331; 0.01371033744276951 0.005627602989682013 ‚Ä¶ -0.000821467707027095 0.0025277126427506714; ‚Ä¶ ; -0.010273994914017084 -0.0017402676438274683 ‚Ä¶ 0.0033771064851509554 0.001475814749556125; 0.008511043430941185 0.006449493358035305 ‚Ä¶ -0.0037750029360778857 0.012244473849873119], [-0.001359465729245611 -0.0011193388674030763 ‚Ä¶ -0.0012247078943883826 -0.0012615332522085338; -0.02027198835690988 -0.020056942374687983 ‚Ä¶ -0.020575428981007206 -0.02019399845374865; -0.00046475243507199896 -0.000468882128447815 ‚Ä¶ 0.00013007153195079187 -9.49077005642371e-5; 0.0016449962407624396 0.0012475814275313814 ‚Ä¶ 0.0014518258489282514 0.0015360566883672604], [0.000443964983575913 0.00044273146189499824 ‚Ä¶ 0.0004398702932028956 0.0004372766188392979; -0.017709892167667267 -0.017712176010874287 ‚Ä¶ -0.01770864632234514 -0.01770925582385858; 0.017912702215229385 0.017908589260831633 ‚Ä¶ 0.01790824779824713 0.01790959602550954], [-0.014184802878778536 -0.0141848062473078 ‚Ä¶ -0.014184814221490632 -0.014184858893310944]], [[0.9999981518529311 0.9999987470805 ‚Ä¶ 0.9999985000905735 0.9999984085338536; 0.9995890464880574 0.9995977190625784 ‚Ä¶ 0.9995766517222475 0.99959220242645; 0.9999997840051741 0.9999997801495496 ‚Ä¶ 0.9999999830813966 0.9999999909925283; 0.9999972939873679 0.9999984435405816 ‚Ä¶ 0.9999978922017044 0.9999976405298501], [0.9999998028950934 0.9999998039888527 ‚Ä¶ 0.9999998065141251 0.9999998087891586; 0.9996863597194096 0.9996862788209598 ‚Ä¶ 0.9996864038454301 0.9996863822581651; 0.9996791350993485 0.9996792824306868 ‚Ä¶ 0.9996792946607965 0.999679246370203], [0.9997987913672902 0.9997987912717263 ‚Ä¶ 0.9997987910455018 0.9997987897781768]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Un po' di parametri degni di questo nome\n",
    "\n",
    "# Creiamo le matrici dei pesi e i vettori dei bias\n",
    "Ws_and_bs =[ [init(n[i+1],n[i]) , init(n[i+1])]  for i=1:N] # The second part of the pair is a vector here\n",
    "\n",
    "X‚ÇÄ = init(n[1],k)         # batch di k pattern (aka i dati)\n",
    "y  = init(n[end],k);      # y is what we will compare X_N against (aka l'etichetta)\n",
    "\n",
    "ùìÅ(x,y)  = sum(abs2,x-y)/2   # loss (errore quadratico)\n",
    "ùìÅ‚Ä≤(x,y) = x .- y;           # derivata della loss (w.r.t. output layer)\n",
    "\n",
    "X, Œ¥ = neural_net(Ws_and_bs,X‚ÇÄ) # Inferenza (aka forward pass)\n",
    "# X e Œ¥ hanno i valori per ogni layer (servono per fare il backward pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a9a2d",
   "metadata": {},
   "source": [
    "Adesso possiamo calcolare il gradiente della loss usando la nostra tecnica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b663117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Matrix{Matrix{Float64}}}:\n",
       " [[2.921837886888091e-8 -1.4711433974972633e-8 ‚Ä¶ 8.321687123215068e-9 3.786571328199305e-11; -1.394753445563111e-7 7.022635947221968e-8 ‚Ä¶ -3.9723331733844425e-8 -1.79618643558014e-10; -1.0866779572546805e-7 5.4714128932074444e-8 ‚Ä¶ -3.0949713876235533e-8 -1.4088794553047937e-10; -5.025187034600354e-8 2.5301714711811154e-8 ‚Ä¶ -1.4312244643745392e-8 -6.51813557114257e-11]; [-3.725536599527891e-6; 1.778386428063789e-5; 1.3855866276020647e-5; 6.4074454460771365e-6;;];;]\n",
       " [[1.4021156294797213e-6 2.4043593639447166e-5 1.2832992243825e-7 -1.6034115944457495e-6; 8.006226231062926e-7 1.3729142259427515e-5 7.327771836586497e-8 -9.155647215308128e-7; -7.703036664355318e-7 -1.3209230271742555e-5 -7.050273756158094e-8 8.808929712731839e-7]; [-0.0011800264282396267; -0.0006738073744923615; 0.0006482908098094921;;];;]\n",
       " [[-4.290091726418208e-5 0.0017225307999695642 -0.0017417754785848884]; [-0.09726197995251608;;];;]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## The diagonal matrix\n",
    "#M = Diagonal([ [‚Ñã(Œ¥[i]) ‚àò ‚Ñõ(X[i-1])  ‚Ñã(Œ¥[i])] for i=1:N]) # OLD!\n",
    "M = Diagonal([ [‚Ñã(Œ¥[i]) ‚àò ‚Ñõ(X[i-1])  ‚Ñã(Œ¥[i]) ‚àò ‚Ñõ(ones(1,k))] for i=1:N])\n",
    "\n",
    "#M = Bidiagonal([‚Ñã(Œ¥[i]) ‚àò ‚Ñõ(X[i-1]) for i=1:N], [‚Ñã(Œ¥[i]) ‚àò ‚Ñõ(ones(1,k)) for i=1:(N-1)], :U)\n",
    "#M = [M [fill(ùí™, N-1); ‚Ñã(Œ¥[N]) ‚àò ‚Ñõ(ones(1,k))]]\n",
    "\n",
    "\n",
    "## The lower triangular matrix (I-L)\n",
    "ImL = Bidiagonal([‚Ñê() for i in 1:N], -[‚Ñã(Œ¥[i]) ‚àò ‚Ñí(Ws_and_bs[i][1]) for i=2:N] , :L)\n",
    "\n",
    "## derivata della loss (rispetto all'output layer)\n",
    "g = [ fill(ùí™(),N-1) ; [ùìÅ‚Ä≤(X[N],y)] ]      \n",
    "\n",
    "## Finalmente, il gradiente della loss, usando il backslash\n",
    "‚àáJ = M' * (ImL' \\ g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74967697",
   "metadata": {},
   "source": [
    "## Indagine del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78e4d4",
   "metadata": {},
   "source": [
    "Vediamo com'√® fatta `M`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec7794f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3√ó3 Diagonal{Matrix{Main.MyOperator.Operator}, Vector{Matrix{Main.MyOperator.Operator}}}:\n",
       " [‚Ñã(4, 10)‚àò‚Ñõ(5, 10) ‚Ñã(4, 10)‚àò‚Ñõ(1, 10)]  ‚Ä¶    ‚ãÖ  \n",
       "   ‚ãÖ                                         ‚ãÖ  \n",
       "   ‚ãÖ                                       [‚Ñã(1, 10)‚àò‚Ñõ(3, 10) ‚Ñã(1, 10)‚àò‚Ñõ(1, 10)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1513fd",
   "metadata": {},
   "source": [
    "`M` √® di tipo `Diagonal`. In posizione $(i,i)$ ha i parametri del layer $i$-esimo: sia i pesi, sia il bias.\n",
    "\n",
    "Nell'articolo originale, ciascun nodo interno del grafo computazionale aveva due nodi entranti: uno corrispondente ai suoi parametri, l'altro corrispondente al layer precedente. \n",
    "\n",
    "Per√≤ avrebbe senso anche dividere i pesi e i bias in due famiglie distinte di nodi (come ho fatto in presentazione). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c845e",
   "metadata": {},
   "source": [
    "Come diventa la `M` in questo caso? E nel codice che succede?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d3c9716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3√ó6 Matrix{Main.MyOperator.Operator}:\n",
       " ‚Ñã(4, 10)‚àò‚Ñõ(5, 10)  ‚Ñã(4, 10)‚àò‚Ñõ(1, 10)  ‚Ä¶  ùí™\n",
       " ùí™                  ùí™                     ùí™\n",
       " ùí™                  ùí™                     ‚Ñã(1, 10)‚àò‚Ñõ(1, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prima versione della nuova M\n",
    "\n",
    "Mnuova = Matrix{eltype(ImL)}(undef, 3,6)\n",
    "\n",
    "for i = 1:size(Mnuova,1), j = 1:size(Mnuova,2)\n",
    "    Mnuova[i, j] = ùí™()\n",
    "    Mnuova[i, 2i-1] = M[i,i][1]\n",
    "    Mnuova[i, 2i] = M[i,i][2]\n",
    "end\n",
    "\n",
    "display(Mnuova)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c57fcd",
   "metadata": {},
   "source": [
    "`M` √® sempre diagonale, ma a blocchi. I pesi e i bias sono in due elementi diversi, stavolta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6dbc308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4√ó10 Matrix{Float64}:\n",
       " -4.02431e-7  -7.69835e-7  -5.68909e-7  ‚Ä¶  -7.54559e-7  -4.86312e-7\n",
       "  1.9218e-6    3.67633e-6   2.71681e-6      3.60338e-6   2.32237e-6\n",
       "  1.4967e-6    2.86314e-6   2.11586e-6      2.80632e-6   1.80867e-6\n",
       "  6.9213e-7    1.32402e-6   9.7845e-7       1.29774e-6   8.36394e-7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3√ó10 Matrix{Float64}:\n",
       " -0.000127466  -0.000243837  -0.000180196  ‚Ä¶  -0.000238999  -0.000154034\n",
       " -7.28072e-5   -0.000139277  -0.000102926     -0.000136513  -8.79827e-5\n",
       "  7.00505e-5    0.000134004   9.90289e-5       0.000131345   8.46515e-5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1√ó10 Matrix{Float64}:\n",
       " -0.0105083  -0.020102  -0.0148554  ‚Ä¶  0.0105197  -0.0197031  -0.0126986"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = (ImL' \\ g)\n",
    "for v in x\n",
    "    display(v)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f35290bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DimensionMismatch: arrays could not be broadcast to a common size: a has axes Base.OneTo(3) and b has axes Base.OneTo(4)"
     ]
    }
   ],
   "source": [
    "try\n",
    "    Mnuova' * x\n",
    "catch e\n",
    "    showerror(stderr, e)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377168fb",
   "metadata": {},
   "source": [
    "Il problema sono gli zeri. Consideriamo ad esempio la prima riga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24f080b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4√ó5 Matrix{Float64}:\n",
       "  2.92184e-8  -1.47114e-8   8.89709e-9   8.32169e-9   3.78657e-11\n",
       " -1.39475e-7   7.02264e-8  -4.24682e-8  -3.97233e-8  -1.79619e-10\n",
       " -1.08668e-7   5.47141e-8  -3.30898e-8  -3.09497e-8  -1.40888e-10\n",
       " -5.02519e-8   2.53017e-8  -1.53019e-8  -1.43122e-8  -6.51814e-11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3√ó10 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1√ó10 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Mnuova'[1,1] * x[1])\n",
    "display(Mnuova'[1,2] * x[2])\n",
    "display(Mnuova'[1,3] * x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c154d",
   "metadata": {},
   "source": [
    "Queste tre matrici andrebbero sommate, per andare a formare il primo elemento di `‚àáJ`. \n",
    "- Chiaramente, non sono compatibili\n",
    "- Eppure, due di questi addendi sono zeri\n",
    "Quindi la somma la si potrebbe (anzi, dovrebbe) evitare!\n",
    "\n",
    "Nel codice originale degli autori, il problema viene evitato ricorrendo a un \"trucco\". Essendo la `M` diagonale, nel fare `M' * x` non vengono fatte somme! Certo, `M[i,i] * x[i]` coinvolge (l'aggiunta di) una `Matrix{Operator}`, ma questo viene preso in carico da una regola apposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In matematica siamo spesso tentati (e spesso abituati) ad ignorare gli zeri, o a considerarli come \"un' unico zero indistinto\". E con buona ragione. Eppure √® una questione assai rilevante, che spesso fa da \"campanello d'allarme\" per una ambiguit√† ancora pi√π grossa.\n",
    "\n",
    "Comunque, a parte la filosofia, nella pratica ha molta importanza la dimensione dello zero, essendoci matrici..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a0ddb",
   "metadata": {},
   "source": [
    "Per poter svolgere questo calcolo si potrebbe, ad esempio:\n",
    "- Trattare lo zero come un \"simbolo\", ovvero non valutarlo mai, e sovraccaricare le operazioni in maniera opportuna\n",
    "- Valutare l'omomorfismo nullo, ma fare un qualche `reshape`. Il problema √® dove\n",
    "- Aggirare il problema, come hanno fatto gli autori"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.4",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
